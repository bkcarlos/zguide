---
weight: 4
title: '4. 可靠的请求-回复模式'
---

# 第4章 - 可靠的请求-回复模式 {#reliable-request-reply}

第3章涵盖了 ZeroMQ 请求-回复模式的高级用法，并提供了工作示例。本章着眼于可靠性的一般问题，并在 ZeroMQ 的核心请求-回复模式之上构建一套可靠的消息模式。

在本章中，我们重点关注用户空间请求-回复*模式*，可重用的模型，帮助你设计自己的 ZeroMQ 架构：

* *懒惰海盗*模式：从客户端进行可靠的请求-回复
* *简单海盗*模式：使用负载均衡的可靠请求-回复
* *偏执海盗*模式：带心跳的可靠请求-回复
* *大管家*模式：面向服务的可靠队列
* *泰坦尼克*模式：基于磁盘/断开连接的可靠队列
* *双星*模式：主备服务器故障转移
* *自由职业者*模式：无代理的可靠请求-回复

## 什么是"可靠性"？ {#What-is-Reliability}

大多数谈论"可靠性"的人实际上并不真正知道他们的意思。我们只能用故障来定义可靠性。也就是说，如果我们能够处理某一组明确定义和理解的故障，那么我们对于这些故障是可靠的。不多也不少。所以让我们看看分布式 ZeroMQ 应用程序中可能的故障原因，大致按照概率递减的顺序：

* 应用程序代码是最严重的问题。它可能崩溃和退出，冻结并停止响应输入，运行得太慢而无法处理输入，耗尽所有内存等等。

* 系统代码——比如我们使用 ZeroMQ 编写的代理——可能因为与应用程序代码相同的原因而死亡。系统代码*应该*比应用程序代码更可靠，但它仍然可能崩溃和烧毁，特别是如果它试图为慢客户端排队消息时可能耗尽内存。

* 消息队列可能溢出，通常发生在已经学会粗暴处理慢客户端的系统代码中。当队列溢出时，它开始丢弃消息。所以我们得到"丢失"的消息。

* 网络可能失败（例如，WiFi 被关闭或超出范围）。在这种情况下，ZeroMQ 会自动重新连接，但与此同时，消息可能会丢失。

* 硬件可能失败，并带走在该机器上运行的所有进程。

* 网络可能以奇特的方式失败，例如，交换机上的某些端口可能死亡，网络的那些部分变得无法访问。

* 整个数据中心可能被闪电、地震、火灾或更平凡的电力或冷却故障击中。

使软件系统对所有这些可能的故障完全可靠是一项极其困难和昂贵的工作，超出了本书的范围。

由于上述列表中的前五种情况涵盖了大公司之外99.9%的现实世界需求（根据我刚刚进行的高度科学研究，该研究还告诉我78%的统计数据是现场编造的，而且永远不要相信我们自己没有伪造的统计数据），这就是我们要检查的内容。如果你是一家有钱花在最后两种情况上的大公司，请立即联系我的公司！我海滩别墅后面有一个大洞等待改造成高管游泳池。

## 设计可靠性 {#Designing-Reliability}

所以为了让事情变得极其简单，可靠性就是"当代码冻结或崩溃时保持事情正常工作"，我们将这种情况简称为"死亡"。然而，我们想要保持正常工作的东西比仅仅消息更复杂。我们需要采用每个核心 ZeroMQ 消息模式，看看如何使其工作（如果我们可以的话），即使代码死亡。

让我们一一看看：

* 请求-回复：如果服务器死亡（在处理请求时），客户端可以发现这一点，因为它不会得到答案。然后它可以愤怒地放弃，等待并稍后重试，找到另一个服务器等等。至于客户端死亡，我们现在可以将其视为"别人的问题"。

* 发布-订阅：如果客户端死亡（在获得一些数据后），服务器不知道。发布-订阅不从客户端向服务器发送任何信息。但客户端可以带外联系服务器，例如，通过请求-回复，并问"请重新发送我错过的一切"。至于服务器死亡，这超出了这里的范围。订阅者也可以自我验证他们没有运行得太慢，如果他们是的话就采取行动（例如，警告操作员并死亡）。

* 管道：如果工作者死亡（在工作时），通风器不知道。管道，就像时间的研磨齿轮，只在一个方向工作。但下游收集器可以检测到一个任务没有完成，并向通风器发送消息说"嘿，重新发送任务324！"如果通风器或收集器死亡，最初发送工作批次的任何上游客户端可能厌倦等待并重新发送整批。这不优雅，但系统代码真的不应该经常死亡到足以重要。

在本章中，我们将只专注于请求-回复，这是可靠消息传递的低垂果实。

基本的请求-回复模式（REQ 客户端套接字对 REP 服务器套接字进行阻塞发送/接收）在处理最常见类型的故障方面得分很低。如果服务器在处理请求时崩溃，客户端只是永远挂起。如果网络丢失请求或回复，客户端永远挂起。

由于 ZeroMQ 能够静默重新连接对等体、负载均衡消息等，请求-回复仍然比 TCP 好得多。但对于真正的工作来说，它仍然不够好。你真正可以信任基本请求-回复模式的唯一情况是在同一进程中的两个线程之间，那里没有网络或单独的服务器进程会死亡。

然而，通过一点额外的工作，这个谦逊的模式成为跨分布式网络进行真正工作的良好基础，我们得到一套可靠的请求-回复（RRR）模式，我喜欢称之为*海盗*模式（我希望你最终会理解这个笑话）。

根据我的经验，大致有三种方式将客户端连接到服务器。每种都需要特定的可靠性方法：

* 多个客户端直接与单个服务器通话。用例：客户端需要与之通话的单个众所周知的服务器。我们旨在处理的故障类型：服务器崩溃和重启，以及网络断开。

* 多个客户端与将工作分发给多个工作者的代理代理通话。用例：面向服务的事务处理。我们旨在处理的故障类型：工作者崩溃和重启、工作者忙循环、工作者过载、队列崩溃和重启，以及网络断开。

* 多个客户端与多个服务器通话，没有中介代理。用例：分布式服务，如名称解析。我们旨在处理的故障类型：服务崩溃和重启、服务忙循环、服务过载，以及网络断开。

这些方法中的每一种都有其权衡，通常你会混合使用它们。我们将详细查看所有三种。

## 客户端可靠性（懒惰海盗模式） {#Client-Side-Reliability-Lazy-Pirate-Pattern}

我们可以通过对客户端的一些更改获得非常简单的可靠请求-回复。我们称之为懒惰海盗模式。我们不是进行阻塞接收，而是：

* 轮询 REQ 套接字，只有当确定回复已到达时才从中接收。
* 如果在超时期间内没有回复到达，则重新发送请求。
* 如果经过几个请求后仍然没有回复，则放弃事务。

如果你试图以严格的发送/接收方式之外的任何方式使用 REQ 套接字，你会得到错误（技术上，REQ 套接字实现一个小的有限状态机来强制发送/接收乒乓，因此错误代码被称为"EFSM"）。当我们想要在海盗模式中使用 REQ 时，这有点令人讨厌，因为我们可能在得到回复之前发送几个请求。

相当好的蛮力解决方案是在错误后关闭并重新打开 REQ 套接字：

```c
//  懒惰海盗客户端
//  使用 zmq_poll 来在超时时进行安全的请求-回复
//  运行此客户端与 hwserver 一起测试

#include "czmq.h"

#define REQUEST_TIMEOUT     2500    //  毫秒，（> 1000！）
#define REQUEST_RETRIES     3       //  放弃之前

static char *
s_recv_from_server (void *client)
{
    char *reply = NULL;
    
    //  轮询套接字进行回复，有超时
    zmq_pollitem_t items [] = { { client, 0, ZMQ_POLLIN, 0 } };
    int rc = zmq_poll (items, 1, REQUEST_TIMEOUT * ZMQ_POLL_MSEC);
    if (rc == -1)
        return NULL;    //  中断

    //  如果我们得到了回复，处理它
    if (items [0].revents & ZMQ_POLLIN)
        reply = zstr_recv (client);

    return reply;
}

int
main (void)
{
    zctx_t *ctx = zctx_new ();
    printf ("I: 连接到 hello world 服务器...\n");
    void *client = zsocket_new (ctx, ZMQ_REQ);
    assert (client);
    zsocket_connect (client, "tcp://localhost:5555");

    int sequence = 0;
    int retries_left = REQUEST_RETRIES;
    while (retries_left && !zctx_interrupted) {
        //  我们发送一个请求，然后我们得到一个回复
        char request [10];
        sprintf (request, "%d", ++sequence);
        zstr_send (client, request);

        char *reply = s_recv_from_server (client);
        if (reply) {
            printf ("I: 服务器回复了 (%s)\n", reply);
            free (reply);
            retries_left = REQUEST_RETRIES;
        }
        else {
            if (--retries_left == 0) {
                printf ("E: 服务器似乎离线了，正在放弃\n");
                break;
            }
            else {
                printf ("W: 服务器没有回复，正在重试...\n");
                //  旧套接字处于混乱状态，关闭它并打开新的
                zsocket_destroy (ctx, client);
                printf ("I: 重新连接到服务器...\n");
                client = zsocket_new (ctx, ZMQ_REQ);
                zsocket_connect (client, "tcp://localhost:5555");
            }
        }
    }
    zctx_destroy (&ctx);
    return 0;
}
```

与匹配的服务器一起运行：

```c
//  懒惰海盗服务器
//  绑定 REQ 套接字到 tcp://*:5555
//  就像 hwserver 一样，除了：
//   - 在某些周期后回显请求号
//   - 随机减慢回复或在某些周期后退出

#include "czmq.h"

int main (void)
{
    srandom ((unsigned) time (NULL));

    zctx_t *ctx = zctx_new ();
    void *server = zsocket_new (ctx, ZMQ_REP);
    zsocket_bind (server, "tcp://*:5555");

    int cycles = 0;
    while (1) {
        char *request = zstr_recv (server);
        cycles++;

        //  在某些周期后模拟各种问题
        if (cycles > 3 && randof (3) == 0) {
            printf ("I: 模拟崩溃\n");
            break;
        }
        else
        if (cycles > 3 && randof (3) == 0) {
            printf ("I: 模拟 CPU 过载\n");
            sleep (2);
        }
        printf ("I: 正常请求 (%s)\n", request);
        sleep (1);                  //  做一些工作
        zstr_send (server, request);
        free (request);
    }
    zctx_destroy (&ctx);
    return 0;
}
```
```
#-----------#   #-----------#   #-----------#
|  Client   |   |  Client   |   |  Client   |
+-----------+   +-----------+   +-----------+
|   Retry   |   |   Retry   |   |   Retry   |
+-----------+   +-----------+   +-----------+
|    REQ    |   |    REQ    |   |    REQ    |
'-----------'   '-----------'   '-----------'
      ^               ^               ^
      |               |               |
      '---------------+---------------'
                      |
                      v
                .----------.
                |   REP    |
                +----------+
                |  Server  |
                #----------#
```

要运行此测试用例，在两个控制台窗口中启动客户端和服务器。服务器将在几条消息后随机行为不当。你可以检查客户端的响应。这是来自服务器的典型输出：

```
I: normal request (1)
I: normal request (2)
I: normal request (3)
I: simulating CPU overload
I: normal request (4)
I: simulating a crash
```

这是客户端的响应：

```
I: connecting to server...
I: server replied OK (1)
I: server replied OK (2)
I: server replied OK (3)
W: no response from server, retrying...
I: connecting to server...
W: no response from server, retrying...
I: connecting to server...
E: server seems to be offline, abandoning
```

客户端为每条消息排序，并检查回复是否完全按顺序返回：没有请求或回复丢失，没有回复返回超过一次，或无序。运行测试几次，直到你确信这种机制实际有效。在生产应用程序中你不需要序列号；它们只是帮助我们信任我们的设计。

客户端使用 REQ 套接字，并进行蛮力关闭/重新打开，因为 REQ 套接字强加了严格的发送/接收周期。你可能会尝试使用 DEALER 代替，但这不是一个好决定。首先，这意味着模拟 REQ 对信封所做的秘密调味料（如果你忘记了那是什么，这是一个好兆头，表明你不想必须这样做）。其次，这意味着可能得到你不期望的回复。

仅在客户端处理故障在我们有一组客户端与单个服务器通话时有效。它可以处理服务器崩溃，但只有在恢复意味着重启同一服务器时。如果有永久错误，例如服务器硬件上的电源故障，这种方法不会工作。因为服务器中的应用程序代码通常是任何架构中最大的故障源，依赖单个服务器不是一个好主意。

所以，优缺点：

* 优点：易于理解和实现。
* 优点：与现有客户端和服务器应用程序代码配合良好。
* 优点：ZeroMQ 自动重试实际重新连接直到它工作。
* 缺点：不故障转移到备份或备用服务器。

## 基本可靠队列（简单海盗模式） {#Basic-Reliable-Queuing-Simple-Pirate-Pattern}

我们的第二种方法扩展了懒惰海盗模式，使用队列代理让我们透明地与多个服务器通话，我们可以更准确地称之为"工作者"。我们将分阶段开发这个，从最小工作模型开始，简单海盗模式。

在所有这些海盗模式中，工作者都是无状态的。如果应用程序需要一些共享状态，比如共享数据库，在我们设计消息传递框架时我们不知道它。有队列代理意味着工作者可以来去而客户端不知道任何关于它的事情。如果一个工作者死亡，另一个接管。这是一个很好的简单拓扑，只有一个真正的弱点，即中央队列本身，这可能成为管理问题和单点故障。

```
#-----------#   #-----------#   #-----------#
|  Client   |   |  Client   |   |  Client   |
+-----------+   +-----------+   +-----------+
|   Retry   |   |   Retry   |   |   Retry   |
+-----------+   +-----------+   +-----------+
|    REQ    |   |    REQ    |   |    REQ    |
'-----+-----'   '-----+-----'   '-----+-----'
      |               |               |
      '---------------+---------------'
                      |
                      v
                .-----------.
                |  ROUTER   |
                +-----------+
                |   Load    |
                |  balancer |
                +-----------+
                |  ROUTER   |
                '-----------'
                      ^
                      |
      .---------------+---------------.
      |               |               |
.-----+-----.   .-----+-----.   .-----+-----.
|    REQ    |   |    REQ    |   |    REQ    |
+-----------+   +-----------+   +-----------+
|   Worker  |   |   Worker  |   |   Worker  |
#-----------#   #-----------#   #-----------#
```

队列代理的基础是第3章中的负载均衡代理。我们需要做的绝对*最少*是什么来处理死亡或阻塞的工作者？结果，这出人意料地少。我们在客户端中已经有重试机制。所以使用负载均衡模式将工作得相当好。这符合 ZeroMQ 的哲学，即我们可以通过在中间插入朴素代理来扩展请求-回复等对等模式。

我们不需要特殊客户端；我们仍然使用懒惰海盗客户端。这是队列，它与负载均衡代理的主要任务相同：

```c
//  简单海盗队列
//  这与我们的负载均衡代理相同

#include "czmq.h"

int main (void)
{
    zctx_t *ctx = zctx_new ();

    //  前端套接字与客户端通话，后端与工作者通话
    void *frontend = zsocket_new (ctx, ZMQ_ROUTER);
    void *backend  = zsocket_new (ctx, ZMQ_ROUTER);
    zsocket_bind (frontend, "tcp://*:5555");    //  为客户端
    zsocket_bind (backend,  "tcp://*:5556");    //  为工作者

    //  工作者队列，每个条目是一个工作者身份
    zlist_t *workers = zlist_new ();

    while (true) {
        zmq_pollitem_t items [] = {
            { backend,  0, ZMQ_POLLIN, 0 },
            { frontend, 0, ZMQ_POLLIN, 0 }
        };
        //  只有工作者可用时才轮询前端
        int rc = zmq_poll (items, zlist_size (workers)? 2: 1, -1);
        if (rc == -1)
            break;              //  中断

        //  处理来自后端的工作者活动
        if (items [0].revents & ZMQ_POLLIN) {
            //  使用工作者身份进行负载均衡
            zmsg_t *msg = zmsg_recv (backend);
            if (!msg)
                break;          //  中断
            zframe_t *identity = zmsg_unwrap (msg);
            zlist_append (workers, identity);

            //  转发消息给客户端（如果需要）
            if (zmsg_size (msg) == 0)
                zmsg_destroy (&msg);
            else
                zmsg_send (&msg, frontend);
        }
        if (items [1].revents & ZMQ_POLLIN) {
            //  获取客户端请求，路由到第一个可用工作者
            zmsg_t *msg = zmsg_recv (frontend);
            if (msg) {
                zmsg_wrap (msg, (zframe_t *) zlist_pop (workers));
                zmsg_send (&msg, backend);
            }
        }
    }
    //  当我们完成时，清理正确
    while (zlist_size (workers)) {
        zframe_t *frame = (zframe_t *) zlist_pop (workers);
        zframe_destroy (&frame);
    }
    zlist_destroy (&workers);
    zctx_destroy (&ctx);
    return 0;
}
```

这是工作者，它采用懒惰海盗服务器并将其适配为负载均衡模式（使用 REQ "ready" 信号）：

```c
//  简单海盗工作者
//  连接 REQ 套接字到 tcp://localhost:5556
//  实现工作者部分的负载均衡

#include "czmq.h"

int main (void)
{
    zctx_t *ctx = zctx_new ();
    void *worker = zsocket_new (ctx, ZMQ_REQ);

    //  设置随机身份使跟踪更容易
    char identity [10];
    sprintf (identity, "%04X-%04X", randof (0x10000), randof (0x10000));
    zmq_setsockopt (worker, ZMQ_IDENTITY, identity, strlen (identity));
    zsocket_connect (worker, "tcp://localhost:5556");

    //  告诉代理我们准备好工作
    printf ("I: (%s) 工作者就绪\n", identity);
    zframe_t *frame = zframe_new (ZFRAME_MORE, 0);
    zframe_send (&frame, worker, 0);

    int cycles = 0;
    while (true) {
        zmsg_t *msg = zmsg_recv (worker);
        if (!msg)
            break;              //  中断

        //  在某些周期后模拟各种问题
        cycles++;
        if (cycles > 3 && randof (5) == 0) {
            printf ("I: (%s) 模拟崩溃\n", identity);
            zmsg_destroy (&msg);
            break;
        }
        else
        if (cycles > 3 && randof (5) == 0) {
            printf ("I: (%s) 模拟 CPU 过载\n", identity);
            sleep (3);
        }
        printf ("I: (%s) 正常回复\n", identity);
        sleep (1);              //  做一些工作
        zmsg_send (&msg, worker);
    }
    zctx_destroy (&ctx);
    return 0;
}
```

为了测试这个，启动一些工作者、一个懒惰海盗客户端和队列，以任何顺序。你会看到工作者最终都崩溃和烧毁，客户端重试然后放弃。队列从不停止，你可以无休止地重启工作者和客户端。这个模型与任意数量的客户端和工作者一起工作。

## 稳健可靠队列（偏执海盗模式） {#Robust-Reliable-Queuing-Paranoid-Pirate-Pattern}

```
#-----------#   #-----------#   #-----------#
|  Client   |   |  Client   |   |  Client   |
+-----------+   +-----------+   +-----------+
|   Retry   |   |   Retry   |   |   Retry   |
+-----------+   +-----------+   +-----------+
|    REQ    |   |    REQ    |   |    REQ    |
'-----+-----'   '-----+-----'   '-----+-----'
      |               |               |
      '---------------+---------------'
                      |
                      v
                .-----------.
                |  ROUTER   |
                +-----------+
                |   Queue   |
                +-----------+
                | Heartbeat |
                +-----------+
                |  ROUTER   |
                '-----------'
                      ^
                      |
      .---------------+---------------.
      |               |               |
.-----+-----.   .-----+-----.   .-----+-----.
|  DEALER   |   |  DEALER   |   |  DEALER   |
+-----------+   +-----------+   +-----------+
| Heartbeat |   | Heartbeat |   | Heartbeat |
+-----------+   +-----------+   +-----------+
|   Worker  |   |   Worker  |   |   Worker  |
#-----------#   #-----------#   #-----------#
```

简单海盗队列模式工作得相当好，特别是因为它只是两个现有模式的组合。不过，它确实有一些弱点：

* 面对队列崩溃和重启时它不稳健。客户端会恢复，但工作者不会。虽然 ZeroMQ 会自动重新连接工作者的套接字，但对于新启动的队列来说，工作者没有发出准备就绪信号，所以不存在。为了解决这个问题，我们必须从队列到工作者进行心跳，以便工作者可以检测到队列何时消失。

* 队列不检测工作者故障，所以如果工作者在空闲时死亡，队列无法将其从工作者队列中移除，直到队列向其发送请求。客户端等待并白白重试。这不是一个关键问题，但这不好。为了使这正常工作，我们从工作者到队列进行心跳，以便队列可以在任何阶段检测到丢失的工作者。

我们将在适当迂腐的偏执海盗模式中修复这些问题。

我们之前为工作者使用了 REQ 套接字。对于偏执海盗工作者，我们将切换到 DEALER 套接字。这有让我们随时发送和接收消息的优势，而不是 REQ 强加的锁步发送/接收。DEALER 的缺点是我们必须做自己的信封管理（重新阅读第3章以了解这个概念的背景）。

我们仍然使用懒惰海盗客户端。这是偏执海盗队列代理。由于代码相当长，我将展示核心概念：

偏执海盗队列扩展了带有工作者心跳的负载均衡模式。心跳是那些"简单"的事情之一，很难做对。当我写偏执海盗示例时，花了大约五个小时才让心跳正常工作。请求-回复链的其余部分可能花了十分钟。特别容易创建"虚假故障"，即当对等体因为心跳没有正确发送而决定它们断开连接时。

## 心跳 {#Heartbeating}

心跳解决了知道对等体是活着还是死亡的问题。这不是 ZeroMQ 特有的问题。TCP 有很长的超时（30分钟左右），这意味着可能无法知道对等体是否已经死亡、断开连接，或者带着一箱伏特加、一个红发女郎和一个大的费用账户去布拉格度周末。

正确进行心跳并不容易。在编写偏执海盗示例时，花了大约五个小时才让心跳正常工作。请求-回复链的其余部分可能花了十分钟。特别容易创建"虚假故障"，即当对等体因为心跳没有正确发送而决定它们断开连接时。

我们将看看人们在 ZeroMQ 心跳中使用的三个主要答案。

### 耸耸肩 {#Shrugging-It-Off}

最常见的方法是根本不进行心跳，并希望最好的结果。许多（如果不是大多数）ZeroMQ 应用程序都这样做。ZeroMQ 通过在许多情况下隐藏对等体来鼓励这一点。这种方法会导致什么问题？

* 当我们在跟踪对等体的应用程序中使用 ROUTER 套接字时，随着对等体断开连接和重新连接，应用程序将泄漏内存（应用程序为每个对等体持有的资源）并变得越来越慢。

* 当我们使用基于 SUB 或 DEALER 的数据接收者时，我们无法区分好的沉默（没有数据）和坏的沉默（另一端死亡）。当接收者知道另一边死亡时，它可以例如切换到备份路由。

* 如果我们使用长时间保持沉默的 TCP 连接，在某些网络中，它会死亡。发送某些东西（技术上，"保活"更多于心跳），将保持网络活跃。

### 单向心跳 {#One-Way-Heartbeats}

第二个选择是每秒左右从每个节点向其对等体发送心跳消息。当一个节点在某个超时内（通常几秒钟）没有听到另一个节点的消息时，它将把该对等体视为死亡。听起来不错，对吧？不幸的是，没有。这在某些情况下有效，但在其他情况下有令人讨厌的边缘情况。

对于发布-订阅，这确实有效，这是你可以使用的唯一模型。SUB 套接字不能与 PUB 套接字对话，但 PUB 套接字可以愉快地向其订阅者发送"我还活着"消息。

作为优化，你可以仅在没有真实数据要发送时才发送心跳。此外，如果网络活动是一个问题（例如，在移动网络上，活动会耗尽电池），你可以越来越慢地发送心跳。只要接收者可以检测到故障（活动的急剧停止），那就没问题。

这种设计的典型问题是：

* 当我们发送大量数据时，它可能不准确，因为心跳会在该数据后面延迟。如果心跳延迟，由于网络拥塞，你可能得到虚假超时和断开连接。因此，始终将*任何*传入数据视为心跳，无论发送者是否优化掉心跳。

* 虽然发布-订阅模式会为消失的接收者丢弃消息，但 PUSH 和 DEALER 套接字会排队它们。所以如果你向死亡的对等体发送心跳，当它回来时，它会得到你发送的所有心跳，这可能是数千个。哇哦！

* 这种设计假设心跳超时在整个网络中是相同的。但这不会准确。一些对等体会想要非常激进的心跳以便快速检测故障。一些会想要非常宽松的心跳，以便让睡眠网络安静并节省电力。

### 乒乓心跳 {#Ping-Pong-Heartbeats}

第三个选择是使用乒乓对话。一个对等体向另一个发送 ping 命令，后者用 pong 命令回复。两个命令都没有任何有效载荷。Ping 和 pong 没有关联。因为"客户端"和"服务器"的角色在某些网络中是任意的，我们通常指定任一对等体实际上都可以发送 ping 并期望响应中的 pong。然而，因为超时取决于动态客户端最了解的网络拓扑，通常是客户端 ping 服务器。

这适用于所有基于 ROUTER 的代理。我们在第二个模型中使用的相同优化使这工作得更好：将任何传入数据视为 pong，并且仅在不发送数据时才发送 ping。

### 偏执海盗的心跳 {#Heartbeating-for-Paranoid-Pirate}

对于偏执海盗，我们选择了第二种方法。这可能不是最简单的选择：如果今天设计这个，我可能会尝试乒乓方法。然而原则是相似的。心跳消息在两个方向异步流动，任一对等体都可以决定另一个"死亡"并停止与其交谈。

在工作者中，这是我们如何处理来自队列的心跳：

* 我们计算一个*活跃度*，这是在决定队列死亡之前我们仍然可以错过多少次心跳。它从三开始，每次我们错过心跳时递减。
* 我们在 `zmq_poll` 循环中等待，每次一秒，这是我们的心跳间隔。
* 如果在那段时间内来自队列有任何消息，我们将活跃度重置为三。
* 如果在那段时间内没有消息，我们倒计时我们的活跃度。
* 如果活跃度达到零，我们认为队列死亡。
* 如果队列死亡，我们销毁套接字，创建新的，并重新连接。
* 为了避免打开和关闭太多套接字，我们在重新连接之前等待一定间隔，每次将间隔加倍直到达到32秒。

这是我们如何处理*到*队列的心跳：

* 我们计算何时发送下一个心跳；这是单个变量，因为我们正在与一个对等体通话，队列。
* 在 `zmq_poll` 循环中，每当我们通过这个时间时，我们向队列发送心跳。

这是工作者的基本心跳代码：

```c
#define HEARTBEAT_LIVENESS  3       //  3-5 是合理的
#define HEARTBEAT_INTERVAL  1000    //  毫秒
#define INTERVAL_INIT       1000    //  初始重连
#define INTERVAL_MAX       32000    //  指数退避后

//  如果活跃度达到零，队列被认为断开连接
size_t liveness = HEARTBEAT_LIVENESS;
size_t interval = INTERVAL_INIT;

//  定期发送心跳
uint64_t heartbeat_at = zclock_time () + HEARTBEAT_INTERVAL;

while (true) {
    zmq_pollitem_t items [] = { { worker,  0, ZMQ_POLLIN, 0 } };
    int rc = zmq_poll (items, 1, HEARTBEAT_INTERVAL * ZMQ_POLL_MSEC);

    if (items [0].revents & ZMQ_POLLIN) {
        //  从队列接收任何消息
        liveness = HEARTBEAT_LIVENESS;
        interval = INTERVAL_INIT;
    }
    else
    if (--liveness == 0) {
        zclock_sleep (interval);
        if (interval < INTERVAL_MAX)
            interval *= 2;
        zsocket_destroy (ctx, worker);
        //  ...
        liveness = HEARTBEAT_LIVENESS;
    }
    //  如果是时候向队列发送心跳
    if (zclock_time () > heartbeat_at) {
        heartbeat_at = zclock_time () + HEARTBEAT_INTERVAL;
        //  向队列发送心跳消息
    }
}
```

队列做同样的事情，但管理每个工作者的过期时间。

以下是你自己心跳实现的一些提示：

* 使用 `zmq_poll` 或反应器作为应用程序主要任务的核心。

* 首先在对等体之间构建心跳，通过模拟故障来测试它，*然后*构建消息流的其余部分。事后添加心跳要棘手得多。

* 使用简单跟踪，即打印到控制台，来使这工作。为了帮助你跟踪对等体之间的消息流，使用像 zmsg 提供的转储方法，并递增编号你的消息，这样你就可以看到是否有间隙。

* 在真实应用程序中，心跳必须是可配置的，通常与对等体协商。一些对等体会想要激进的心跳，低至10毫秒。其他对等体会很远，想要心跳高达30秒。

* 如果你对不同对等体有不同的心跳间隔，你的轮询超时应该是这些的最低（最短时间）。不要使用无限超时。

* 在用于消息的同一套接字上进行心跳，这样你的心跳也作为*保活*防止网络连接变陈旧（一些防火墙对静默连接可能不友好）。

## 合同和协议 {#Contracts-and-Protocols}

如果你注意的话，你会意识到由于心跳，偏执海盗与简单海盗不可互操作。但我们如何定义"可互操作"？为了保证互操作性，我们需要一种合同，一种让不同时间和地点的不同团队编写保证一起工作的代码的协议。我们称之为"协议"。

没有规范进行实验很有趣，但这不是真实应用程序的明智基础。如果我们想用另一种语言编写工作者会怎样？我们必须读代码来看事情如何工作吗？如果我们因为某种原因想改变协议会怎样？即使是简单的协议，如果它成功，也会演变并变得更复杂。

缺少合同是一次性应用程序的确定迹象。所以让我们为这个协议写一个合同。我们怎么做？

在 [http://rfc.zeromq.org rfc.zeromq.org] 有一个 wiki，我们特别制作作为公共 ZeroMQ 合同的家。要创建新规范，如果需要在 wiki 上注册，并按照说明操作。这相当直接，尽管写技术文本不是每个人都喜欢的。

我花了大约十五分钟起草新的[海盗模式协议](http://rfc.zeromq.org/spec:6)。这不是一个大规范，但它确实捕获了足够作为争论的基础（"你的队列不兼容 PPP；请修复它！"）。

将 PPP 转变为真正的协议需要更多工作：

* READY 命令中应该有协议版本号，以便可能区分不同版本的 PPP。

* 现在，READY 和 HEARTBEAT 与请求和回复并不完全不同。为了使它们不同，我们需要包含"消息类型"部分的消息结构。

## 面向服务的可靠队列（大管家模式） {#Service-Oriented-Reliable-Queuing-Majordomo-Pattern}

```
#-----------#   #-----------#   #-----------#
|  Client   |   |  Client   |   |  Client   |
'-----+-----'   '-----+-----'   '-----+-----'
      |               |               |
      '---------------+---------------'
"Give me coffee"      |         "Give me tea"
                      v
                .-----------.
                |  Broker   |
                '-----------'
                      ^
                      |
      .---------------+---------------.
      |               |               |
.-----+-----.   .-----+-----.   .-----+-----.
|  "Water"  |   |   "Tea"   |   | "Coffee"  |
+-----------+   +-----------+   +-----------+
|  Worker   |   |  Worker   |   |  Worker  |
#-----------#   #-----------#   #-----------#
```

进步的好处是当律师和委员会不参与时它发生得多快。[单页 MDP 规范](http://rfc.zeromq.org/spec:7)将 PPP 变成更坚固的东西。这是我们应该设计复杂架构的方式：首先写下合同，*然后*编写软件来实现它们。

大管家协议（MDP）以一种有趣的方式扩展和改进了 PPP：它向客户端发送的请求添加"服务名称"，并要求工作者注册特定服务。添加服务名称将我们的偏执海盗队列变成面向服务的代理。MDP 的好处是它来自工作代码、更简单的祖先协议（PPP）和一组精确的改进，每个都解决了明确的问题。这使得起草变得容易。

为了实现大管家，我们需要为客户端和工作者编写框架。要求每个应用程序开发人员阅读规范并使其工作，当他们可以使用为他们工作的更简单 API 时，这真的不明智。

所以虽然我们的第一个合同（MDP本身）定义了我们分布式架构的部分如何彼此交谈，我们的第二个合同定义了用户应用程序如何与我们要设计的技术框架交谈。

大管家有两半，客户端侧和工作者侧。因为我们将编写客户端和工作者应用程序，我们需要两个 API。这是客户端 API 的草图，使用简单的面向对象方法：

```c
mdcli_t *mdcli_new     (char *broker);
void     mdcli_destroy (mdcli_t **self_p);
zmsg_t  *mdcli_send    (mdcli_t *self, char *service, zmsg_t **request_p);
```

就是这样。我们打开到代理的会话，发送请求消息，得到回复消息，最终关闭连接。这是工作者 API 的草图：

```c
mdwrk_t *mdwrk_new     (char *broker,char *service);
void     mdwrk_destroy (mdwrk_t **self_p);
zmsg_t  *mdwrk_recv    (mdwrk_t *self, zmsg_t *reply);
```

它或多或少是对称的，但工作者对话有点不同。工作者第一次做 recv() 时，它传递空回复。此后，它传递当前回复，并得到新请求。

客户端和工作者 API 相当容易构建，因为它们很大程度上基于我们已经开发的偏执海盗代码。

让我们看看客户端 API 在行动中的样子，有一个做10万次请求-回复循环的示例测试程序：

```c
//  大管家协议客户端示例
//  使用 mdcli API 隐藏所有 MDP 协议复杂性

#include "mdcliapi.h"

int main (int argc, char *argv [])
{
    int verbose = (argc > 1 && streq (argv [1], "-v"));
    mdcli_t *session = mdcli_new ("tcp://localhost:5555", verbose);

    int count;
    for (count = 0; count < 100000; count++) {
        zmsg_t *request = zmsg_new ();
        zmsg_pushstr (request, "Hello world");
        zmsg_t *reply = mdcli_send (session, "echo", &request);
        if (reply)
            zmsg_destroy (&reply);
        else
            break;              //  由于中断，中断请求-回复
    }
    printf ("%d 请求/回复已处理\n", count);
    mdcli_destroy (&session);
    return 0;
}
```

让我们看看工作者 API 在行动中的样子，有一个实现回声服务的示例测试程序：

```c
//  大管家协议工作者示例
//  使用 mdwrk API 隐藏所有 MDP 协议复杂性

#include "mdwrkapi.h"

int main (int argc, char *argv [])
{
    int verbose = (argc > 1 && streq (argv [1], "-v"));
    mdwrk_t *session = mdwrk_new (
        "tcp://localhost:5555", "echo", verbose);

    zmsg_t *reply = NULL;
    while (true) {
        zmsg_t *request = mdwrk_recv (session, &reply);
        if (request == NULL)
            break;              //  由于中断，工作者被杀死
        reply = request;        //  回声很简单
    }
    mdwrk_destroy (&session);
    return 0;
}
```

关于工作者 API 代码需要注意的一些事情：

* API 是单线程的。例如，这意味着工作者不会在后台发送心跳。令人高兴的是，这正是我们想要的：如果工作者应用程序卡住，心跳将停止，代理将停止向工作者发送请求。

* 工作者 API 不做指数退避；额外的复杂性不值得。

* API 不做任何错误报告。如果某些东西不如期望，它们引发断言（或异常，取决于语言）。这对参考实现是理想的，所以任何协议错误立即显示。对于真实应用程序，API 应该对无效消息稳健。

你可能想知道为什么工作者 API 手动关闭其套接字并打开新的，当如果对等体消失并回来 ZeroMQ 会自动重新连接套接字。回顾简单海盗和偏执海盗工作者来理解。虽然如果代理死亡并回来 ZeroMQ 会自动重新连接工作者，这不足以用代理重新注册工作者。我知道至少两个解决方案。最简单的，我们在这里使用，是工作者使用心跳监控连接，如果它决定代理死亡，关闭其套接字并用新套接字重新开始。替代方案是代理在从工作者得到心跳时挑战未知工作者并要求它们重新注册。这需要协议支持。

现在让我们设计大管家代理。其核心结构是一组队列，每个服务一个。我们将在工作者出现时创建这些队列（我们可以在工作者消失时删除它们，但现在忘记这个，因为它变得复杂）。此外，我们为每个服务保持工作者队列。

这是代理的核心实现概念。这是迄今为止我们见过的最复杂的示例。完整的大管家代理实现接近500行代码。编写这个并使其有点稳健花了两天。然而，对于全面向服务代理来说，这仍然是一段简短的代码。

## 异步大管家模式 {#Asynchronous-Majordomo-Pattern}

前一节中的大管家实现是简单而愚蠢的。客户端只是原始的简单海盗，包装在一个吸引人的 API 中。当我在测试机器上启动客户端、代理和工作者时，它可以在大约14秒内处理100,000个请求。这部分是由于代码，它愉快地复制消息帧，就好像 CPU 周期是免费的一样。但真正的问题是我们在做网络往返。ZeroMQ 禁用了[Nagle 算法](http://en.wikipedia.org/wiki/Nagles_algorithm)，但往返仍然很慢。

理论在理论上很好，但在实践中，实践更好。让我们用一个简单的测试程序来测量往返的实际成本。这发送一堆消息，首先等待每个消息的回复，其次作为一个批次，将所有回复作为一个批次读回。两种方法都做相同的工作，但它们给出非常不同的结果。我们模拟一个客户端、代理和工作者：

{{< examples name="tripping" title="往返演示器" >}}

在我的开发机器上，这个程序说：

```
Setting up test...
Synchronous round-trip test...
 9057 calls/second
Asynchronous round-trip test...
 173010 calls/second
```

注意客户端线程在开始前做了一个小暂停。这是为了绕过路由器套接字的"特性"之一：如果你发送一个带有尚未连接的对等体地址的消息，消息会被丢弃。在这个例子中，我们不使用负载平衡机制，所以没有睡眠，如果工作者线程连接太慢，它会丢失消息，使我们的测试变得混乱。

正如我们看到的，在最简单的情况下，往返比异步的"尽可能快地将其推入管道"方法慢20倍。让我们看看我们是否可以将此应用于大管家以使其更快。

首先，我们修改客户端 API 以在两个单独的方法中发送和接收：

{{< fragment name="mdclient-async" >}}
mdcli_t *mdcli_new     (char *broker);
void     mdcli_destroy (mdcli_t **self_p);
int      mdcli_send    (mdcli_t *self, char *service, zmsg_t **request_p);
zmsg_t  *mdcli_recv    (mdcli_t *self);
{{< /fragment >}}

将同步客户端 API 重构为异步的字面上只需要几分钟的工作：

{{< examples name="mdcliapi2" title="大管家异步客户端 API" >}}

差异是：

* 我们使用 DEALER 套接字而不是 REQ，所以我们在每个请求和每个响应之前用空分隔符帧模拟 REQ。
* 我们不重试请求；如果应用程序需要重试，它可以自己做这个。
* 我们将同步的 <tt>send</tt> 方法分解为单独的 <tt>send</tt> 和 <tt>recv</tt> 方法。
* <tt>send</tt> 方法是异步的，在发送后立即返回。因此，调用者可以在获得响应之前发送多个消息。
* <tt>recv</tt> 方法等待（有超时）一个响应并将其返回给调用者。

这是相应的客户端测试程序，它发送100,000条消息，然后接收100,000条：

{{< examples name="mdclient2" title="大管家客户端应用程序" >}}

代理和工作者没有改变，因为我们根本没有修改协议。我们看到性能的立即改进。这是同步客户端处理100K请求-回复周期：

```
$ time mdclient
100000 requests/replies processed

real    0m14.088s
user    0m1.310s
sys     0m2.670s
```

这是异步客户端，有一个工作者：

```
$ time mdclient2
100000 replies received

real    0m8.730s
user    0m0.920s
sys     0m1.550s
```

快两倍。不错，但让我们启动10个工作者，看看它如何处理流量：

```
$ time mdclient2
100000 replies received

real    0m3.863s
user    0m0.730s
sys     0m0.470s
```

它不是完全异步的，因为工作者在严格的最后使用基础上获得它们的消息。但它将与更多工作者更好地扩展。在我的 PC 上，八个左右的工作者之后，它不会变得更快。四个内核只能延伸到这里。但我们只用几分钟的工作就获得了4倍的吞吐量改进。代理仍然未优化。它花费大部分时间复制消息帧，而不是做零拷贝，这是它可以做的。但我们得到每秒25K可靠请求/回复调用，努力相当低。

然而，异步大管家模式并不全是玫瑰。它有一个根本弱点，即没有更多工作它无法在代理崩溃后存活。如果你看 <tt>mdcliapi2</tt> 代码，你会看到它在失败后不尝试重新连接。适当的重新连接需要以下内容：

* 每个请求上的数字和每个回复上的匹配数字，这理想地需要协议的更改来强制执行。
* 在客户端 API 中跟踪和保持所有未完成的请求，即那些尚未收到回复的请求。
* 在故障转移的情况下，客户端 API 向代理*重新发送*所有未完成的请求。

这不是一个交易破坏者，但它确实表明性能通常意味着复杂性。这对大管家值得做吗？这取决于你的用例。对于你每个会话调用一次的名称查找服务，不。对于服务数千客户端的 web 前端，可能是的。

## 服务发现 {#Service-Discovery}

所以，我们有一个很好的面向服务的代理，但我们没有办法知道特定服务是否可用。我们知道请求是否失败，但我们不知道为什么。能够询问代理"回声服务正在运行吗？"是有用的。最明显的方法是修改我们的 MDP/Client 协议以添加命令来询问这个。但 MDP/Client 有简单的巨大魅力。向其添加服务发现会使其与 MDP/Worker 协议一样复杂。

试试在运行和不运行工作者的情况下运行这个程序，你应该看到小程序分别报告"200"或"404"。我们示例代理中MMI的实现比较简陋。例如，如果一个工作者消失了，服务仍然显示"存在"。在实践中，代理应该在某个可配置的超时时间后移除没有工作者的服务。

## 幂等服务 {#Idempotent-Services}

幂等性不是你吃药就能获得的。它的意思是重复一个操作是安全的。查看时钟是幂等的。把信用卡借给孩子们不是。虽然许多客户端到服务器的用例是幂等的，但有些不是。幂等用例的例子包括：

* 无状态任务分发，即服务器是无状态工作者的管道，它们纯粹基于请求提供的状态来计算回复。在这种情况下，多次执行相同的请求是安全的（尽管低效）。

* 将逻辑地址转换为要绑定或连接的端点的名称服务。在这种情况下，多次进行相同的查找请求是安全的。

这里是非幂等用例的例子：

* 日志服务。人们不希望相同的日志信息被记录多次。

* 任何对下游节点有影响的服务，例如向其他节点发送信息。如果该服务多次收到相同的请求，下游节点将收到重复的信息。

* 任何以非幂等方式修改共享数据的服务；例如，借记银行账户的服务在没有额外工作的情况下不是幂等的。

当我们的服务器应用程序不是幂等的时，我们必须更仔细地考虑它们可能在什么时候崩溃。如果应用程序在空闲时或在处理请求时死掉，通常是可以的。我们可以使用数据库事务来确保借记和贷记总是一起完成，如果都要完成的话。如果服务器在发送回复时死掉，那就有问题了，因为就它而言，它已经完成了工作。

如果网络在回复返回客户端的过程中死掉，同样的问题就出现了。客户端会认为服务器死了，会重新发送请求，服务器会做同样的工作两次，这不是我们想要的。

为了处理非幂等操作，使用检测和拒绝重复请求的相当标准的解决方案。这意味着：

* 客户端必须用唯一的客户端标识符和唯一的消息编号标记每个请求。

* 服务器在发送回复之前，使用客户端ID和消息编号的组合作为键来存储它。

* 服务器在从给定客户端获得请求时，首先检查它是否有该客户端ID和消息编号的回复。如果有，它不处理请求，而只是重新发送回复。

## 断连可靠性（泰坦尼克模式） {#Disconnected-Reliability-Titanic-Pattern}

一旦你意识到大管家是一个"可靠的"消息代理，你可能会想添加一些旋转磁盘（即基于铁的硬盘盘片）。毕竟，这对所有企业消息系统都有效。这是一个如此诱人的想法，以至于不得不对它持否定态度有点令人沮丧。但残酷的愤世嫉妒是我的专长之一。因此，你不希望基于磁盘的代理位于架构中心的一些原因是：

* 如你所见，懒惰海盗客户端的表现出人意料地好。它适用于从直接客户端到服务器到分布式队列代理的整个架构范围。它确实倾向于假设工作者是无状态和幂等的。但我们可以在不诉诸磁盘的情况下解决这个限制。

* 磁盘带来了一整套问题，从性能缓慢到你必须管理、修复和处理凌晨6点恐慌的额外部件，因为它们不可避免地在日常操作开始时崩溃。海盗模式的总体美妙之处在于它们的简单性。它们不会崩溃。如果你仍然担心硬件，你可以转移到一个根本没有代理的点对点模式。我将在本章后面解释。

话虽如此，基于磁盘的可靠性有一个理智的用例，那就是异步断连网络。它解决了海盗的一个主要问题，即客户端必须实时等待答案。如果客户端和工作者只是偶尔连接（把电子邮件作为类比），我们不能在客户端和工作者之间使用无状态网络。我们必须把状态放在中间。

所以，这是泰坦尼克模式，其中我们将消息写入磁盘以确保它们永远不会丢失，无论客户端和工作者多么偶尔连接。正如我们为服务发现所做的，我们将在MDP之上分层泰坦尼克，而不是扩展它。这非常懒惰，因为这意味着我们可以在专门的工作者中实现我们的发送-忘记可靠性，而不是在代理中。这在几个方面都很出色：

* 这*更*容易，因为我们分而治之：代理处理消息路由，工作者处理可靠性。
* 它让我们混合用一种语言编写的代理和用另一种语言编写的工作者。
* 它让我们独立地发展发送-忘记技术。

唯一的缺点是代理和硬盘之间有额外的网络跳跃。好处很容易值得。

有许多方法可以制作持久的请求-回复架构。我们将瞄准一个简单且无痛的。在玩了几个小时后，我能想出的最简单的设计是一个"代理服务"。也就是说，泰坦尼克根本不影响工作者。如果客户端想要立即回复，它直接与服务对话并希望服务可用。如果客户端乐意等一会儿，它转而与泰坦尼克对话并询问，"嘿，伙计，你能在我去买杂货的时候为我处理这个吗？"

泰坦尼克因此既是工作者又是客户端。客户端和泰坦尼克之间的对话是这样的：

* 客户端：请为我接受这个请求。泰坦尼克：好的，完成了。
* 客户端：你有给我的回复吗？泰坦尼克：是的，在这里。或者，不，还没有。
* 客户端：好的，你现在可以清除那个请求，我很满意。泰坦尼克：好的，完成了。

而泰坦尼克与代理和工作者之间的对话是这样的：

* 泰坦尼克：嘿，代理，有咖啡服务吗？代理：嗯，是的，似乎有。
* 泰坦尼克：嘿，咖啡服务，请为我处理这个。
* 咖啡：当然，给你。
* 泰坦尼克：太棒了！

你可以通过这个和可能的失败场景。如果工作者在处理请求时崩溃，泰坦尼克无限重试。如果回复在某处丢失，泰坦尼克将重试。如果请求被处理但客户端没有得到回复，它会再次询问。如果泰坦尼克在处理请求或回复时崩溃，客户端将再次尝试。只要请求完全提交到安全存储，工作就不会丢失。

握手是书面化的，但可以流水线化，即客户端可以使用异步大管家模式做大量工作，然后稍后获得响应。

我们需要某种方法让客户端请求*其*回复。我们将有许多客户端请求相同的服务，客户端消失并以不同的身份重新出现。这是一个简单的、相当安全的解决方案：

* 每个请求生成一个通用唯一标识符（UUID），泰坦尼克在排队请求后将其返回给客户端。
* 当客户端请求回复时，它必须指定原始请求的UUID。

在现实情况下，客户端希望安全地存储其请求UUID，例如在本地数据库中。

在我们跳跃并编写另一个正式规范（有趣，有趣！）之前，让我们考虑客户端如何与泰坦尼克对话。一种方式是使用单一服务并向其发送三种不同的请求类型。另一种方式，看起来更简单，是使用三种服务：

* `titanic.request`：存储请求消息，并返回请求的UUID。
* `titanic.reply`：为给定的请求UUID获取回复（如果可用）。
* `titanic.close`：确认回复已被存储和处理。

我们只是制作一个多线程工作者，正如我们从ZeroMQ的多线程经验中看到的，这是微不足道的。但是，让我们首先草拟泰坦尼克在ZeroMQ消息和帧方面看起来是什么样的。这给了我们[泰坦尼克服务协议（TSP）](http://rfc.zeromq.org/spec:9)。

使用TSP对客户端应用程序来说显然比通过MDP直接访问服务更费力。这是最短的健壮"echo"客户端示例：

{{< examples name="ticlient" title="Titanic client example" >}}

当然，这可以并且应该被包装在某种框架或API中。要求普通应用程序开发人员学习消息传递的全部细节是不健康的：它会伤害他们的大脑，花费时间，并提供太多制造错误复杂性的方法。此外，它使添加智能变得困难。

例如，这个客户端在每个请求上阻塞，而在真实应用程序中，我们希望在执行任务时做有用的工作。这需要一些非平凡的管道来构建后台线程并与其干净地对话。这是你想要包装在一个平均开发者不能误用的好的简单API中的那种东西。这是我们用于大管家的相同方法。

这是泰坦尼克实现。该服务器使用三个线程处理三个服务，如建议的。它使用最残酷的方法对磁盘进行完全持久化：每条消息一个文件。它如此简单，令人害怕。唯一复杂的部分是它保持所有请求的单独队列，以避免一遍又一遍地读取目录：

{{< examples name="titanic" title="Titanic broker example" >}}

要测试这个，启动`mdbroker`和`titanic`，然后运行`ticlient`。现在任意启动`mdworker`，你应该看到客户端得到响应并愉快地退出。

关于这段代码的一些注意事项：

* 注意一些循环从发送开始，其他从接收消息开始。这是因为泰坦尼克在不同角色中既充当客户端又充当工作者。
* 泰坦尼克代理使用MMI服务发现协议仅向似乎正在运行的服务发送请求。由于我们小大管家代理中MMI实现相当差，这不会总是工作。
* 我们使用inproc连接将新请求数据从`titanic.request`服务发送到主调度器。这节省了调度器扫描磁盘目录、加载所有请求文件并按日期/时间排序的工作。

关于这个例子的重要事情不是性能（虽然我没有测试过，但肯定很糟糕），而是它如何很好地实现可靠性合同。要尝试它，启动mdbroker和titanic程序。然后启动ticlient，然后启动mdworker echo服务。你可以使用`-v`选项运行所有这四个来进行详细活动跟踪。你可以停止并重启任何部分*除了客户端*，什么都不会丢失。

如果你想在真实情况下使用泰坦尼克，你会迅速问"我们如何让这个更快？"

这是我会做的，从示例实现开始：

* 为所有数据使用单个磁盘文件，而不是多个文件。操作系统通常更好地处理几个大文件而不是许多较小的文件。
* 将该磁盘文件组织为循环缓冲区，以便新请求可以连续写入（偶尔进行环绕）。一个线程，全速写入磁盘文件，可以快速工作。
* 将索引保存在内存中，并在启动时从磁盘缓冲区重建索引。这节省了保持索引在磁盘上完全安全所需的额外磁盘头抖动。你希望在每条消息后进行fsync，或者如果你准备在系统故障情况下丢失最后M条消息，则每N毫秒进行一次。
* 使用固态硬盘而不是旋转的氧化铁盘片。
* 预分配整个文件，或以大块分配，这允许循环缓冲区根据需要增长和收缩。这避免了碎片化并确保大多数读取和写入是连续的。

等等。我不推荐的是将消息存储在数据库中，甚至不是"快速"键/值存储，除非你真的喜欢特定的数据库并且没有性能担忧。你将为抽象付出高昂的代价，比原始磁盘文件高十到一千倍。

如果你想让泰坦尼克*更加可靠*，将请求复制到第二个服务器，你会把它放在第二个位置，远到足以在你的主要位置遭受核攻击时幸存，但又不至于太远以至于延迟太大。

如果你想让泰坦尼克*更快但不那么可靠*，纯粹在内存中存储请求和回复。这将给你一个断连网络的功能，但请求不会在泰坦尼克服务器本身崩溃后幸存。

## 高可用性对（二进制星模式） {#High-Availability-Pair-Binary-Star-Pattern}

二进制星模式将两个服务器放在主备高可用性对中。在任何给定时间，其中一个（活动的）接受来自客户端应用程序的连接。另一个（被动的）什么都不做，但两个服务器互相监控。如果活动的从网络中消失，在一定时间后，被动的接管为活动的。

我们在iMatix为我们的[OpenAMQ服务器](http://www.openamq.org)开发了二进制星模式。我们设计它：

* 提供直接的高可用性解决方案。
* 足够简单以实际理解和使用。
* 在需要时可靠地故障转移，只在需要时。

假设我们有一个运行的二进制星对，以下是将导致故障转移的不同场景：

* 运行主服务器的硬件有致命问题（电源爆炸，机器着火，或有人简单地错误地拔掉它），并消失。应用程序看到这个，并重新连接到备份服务器。
* 主服务器所在的网络段崩溃——也许路由器被电源尖峰击中——应用程序开始重新连接到备份服务器。
* 主服务器崩溃或被操作员杀死并且不自动重启。

从故障转移中恢复的工作如下：

* 操作员重启主服务器并修复导致它从网络中消失的任何问题。
* 操作员在对应用程序造成最小干扰的时刻停止备份服务器。
* 当应用程序重新连接到主服务器时，操作员重启备份服务器。

恢复（到使用主服务器作为活动的）是手动操作。痛苦的经验告诉我们，自动恢复是不可取的。有几个原因：

* 故障转移对应用程序造成服务中断，可能持续10-30秒。如果有真正的紧急情况，这比完全中断要好得多。但如果恢复造成进一步的10-30秒中断，最好在非高峰期，当用户已经离开网络时发生。

* 当有紧急情况时，对于那些试图修复问题的人来说，绝对的第一优先级是确定性。自动恢复为系统管理员创造了不确定性，他们不再能够确定哪个服务器负责而不需要双重检查。

* 自动恢复可以创造网络故障转移然后恢复的情况，使操作员处于分析发生了什么的困难位置。有服务中断，但原因不清楚。

话虽如此，如果主服务器再次运行且备份服务器失败，二进制星模式将故障回到主服务器。实际上，这是我们如何引发恢复的。

二进制星对的关闭过程是：

1. 停止被动服务器，然后在任何后来的时间停止活动服务器，或
1. 在任何顺序中停止两个服务器，但在彼此的几秒钟内。

先停止活动服务器，然后停止被动服务器，延迟时间超过故障转移超时时间，将导致应用程序断开连接，然后重新连接，然后再次断开连接，这可能会打扰用户。

### 详细要求 {#Detailed-Requirements}

二进制星尽可能简单，同时仍然准确工作。实际上，当前设计是第三次完整重新设计。我们发现之前的每个设计都太复杂，试图做太多，我们剥离功能直到我们得到一个可理解、易于使用且足够可靠以值得使用的设计。

这些是我们对高可用性架构的要求：

* 故障转移旨在提供针对灾难性系统故障的保险，例如硬件故障、火灾、事故等。有更简单的方法从普通服务器崩溃中恢复，我们已经涵盖了这些。

* 可靠性是通过简单性和可预测性来实现的。设计应该足够简单，以便在故障时可以快速理解和修复。

* 故障转移时间应在60秒以下，最好在10秒以下。

* 故障转移必须自动发生，而恢复必须手动发生。我们希望应用程序自动切换到备份服务器，但我们不希望它们切换回主服务器，除非操作员已经修复了任何问题并决定现在是再次中断应用程序的好时机。

* 客户端应用程序的语义应该简单且易于开发人员理解。理想情况下，它们应该隐藏在客户端API中。

* 对于网络架构师，应该有明确的指令来避免可能导致*脑裂综合症*的设计，在这种情况下，二进制星对中的两个服务器都认为自己是活动服务器。

* 不应该依赖两个服务器的启动顺序。

* 必须能够在不停止客户端应用程序的情况下对任一服务器进行计划停机和重启（尽管它们可能被迫重新连接）。

* 操作员必须能够始终监控两个服务器。

* 必须能够使用高速专用网络连接连接两个服务器。也就是说，故障转移同步必须能够使用特定的IP路由。

我们做出以下假设：

* 单个备份服务器提供足够的保险；我们不需要多级备份。

* 主服务器和备份服务器同样能够承载应用程序负载。我们不尝试在服务器之间平衡负载。

* 有足够的预算来覆盖一个在几乎所有时间都什么都不做的完全冗余备份服务器。

我们不尝试涵盖以下内容：

* 使用活跃备份服务器或负载平衡。在二进制星对中，备份服务器是非活跃的，在主服务器离线之前不做任何有用的工作。

* 以任何方式处理持久消息或事务。我们假设存在不可靠（可能不可信）服务器或二进制星对的网络。

* 任何自动网络探索。二进制星对在网络中手动和明确定义，并为应用程序所知（至少在它们的配置数据中）。

* 服务器之间的状态或消息复制。所有服务器端状态必须在应用程序故障转移时由应用程序重新创建。

这是我们在二进制星中使用的关键术语：

* *主要的*：通常或最初活跃的服务器。

* *备份*：通常被动的服务器。如果主服务器从网络中消失，当客户端应用程序要求备份服务器连接时，它将变为活跃。

* *活跃*：接受客户端连接的服务器。最多有一个活跃服务器。

* *被动*：如果活跃服务器消失则接管的服务器。注意当二进制星对正常运行时，主服务器是活跃的，备份是被动的。当故障转移发生时，角色被交换。

要配置二进制星对，你需要：

1. 告诉主服务器备份服务器的位置。
1. 告诉备份服务器主服务器的位置。
1. 可选地，调整故障转移响应时间，两个服务器必须相同。

主要的调优关注点是你希望服务器多频繁地检查它们的对等状态，以及你希望多快激活故障转移。在我们的例子中，故障转移超时值默认为2000毫秒。如果你减少这个值，备份服务器将更快地接管为活跃，但可能在主服务器可以恢复的情况下接管。例如，你可能已经将主服务器包装在一个shell脚本中，如果它崩溃就重启它。在这种情况下，超时应该高于重启主服务器所需的时间。

为了使客户端应用程序与二进制星对正常工作，它们必须：

1. 知道两个服务器地址。
1. 尝试连接到主服务器，如果失败，则连接到备份服务器。
1. 检测失败的连接，通常使用心跳。
1. 尝试重新连接到主服务器，然后是备份（按此顺序），重试之间的延迟至少与服务器故障转移超时一样高。
1. 重新创建它们在服务器上需要的所有状态。
1. 如果消息需要可靠，则重新传输在故障转移期间丢失的消息。

这不是简单的工作，我们通常会将其包装在一个API中，对真正的最终用户应用程序隐藏。

这些是二进制星模式的主要限制：

* 服务器进程不能是多个二进制星对的一部分。
* 主服务器可以有一个备份服务器，不能更多。
* 被动服务器不做有用的工作，因此被浪费了。
* 备份服务器必须能够处理完整的应用程序负载。
* 故障转移配置不能在运行时修改。
* 客户端应用程序必须做一些工作才能从故障转移中受益。

### 防止脑裂综合症 {#Preventing-Split-Brain-Syndrome}

*脑裂综合症*发生在集群的不同部分同时认为它们是活跃的时候。它导致应用程序停止看到彼此。二进制星有一个检测和消除脑裂的算法，该算法基于三方决策机制（服务器在得到应用程序连接请求且看不到其对等服务器之前不会决定变为活跃）。

然而，仍然可能（错误地）设计网络来愚弄这个算法。一个典型的场景是分布在两栋建筑物之间的二进制星对，每栋建筑物也有一组应用程序，两栋建筑物之间有单一网络链接。破坏这个链接将创建两组客户端应用程序，每组都有二进制星对的一半，每个故障转移服务器将变为活跃。

为了防止脑裂情况，我们必须使用专用网络链接连接二进制星对，这可以简单到将它们都插入同一个交换机，或者更好的是，在两台机器之间直接使用交叉电缆。

我们不能将二进制星架构分割成两个岛屿，每个都有一组应用程序。虽然这可能是一种常见的网络架构类型，但在这种情况下你应该使用联邦，而不是高可用性故障转移。

适当偏执的网络配置将使用两个私有集群互连，而不是单一的。此外，用于集群的网卡将不同于用于消息流量的网卡，甚至可能在服务器硬件的不同路径上。目标是将网络中可能的故障与集群中可能的故障分开。网络端口可能有相对高的故障率。

### 二进制星实现 {#Binary-Star-Implementation}

不再啰嗦，这是二进制星服务器的概念验证实现。主服务器和备份服务器运行相同的代码，你在运行代码时选择它们的角色：

{{< examples name="bstarsrv" title="Binary Star server" >}}

这是客户端：

{{< examples name="bstarcli" title="Binary Star client" >}}

要测试二进制星，以任何顺序启动服务器和客户端：

```
bstarsrv -p     # 启动主服务器
bstarsrv -b     # 启动备份服务器
bstarcli
```

然后你可以通过杀死主服务器来引发故障转移，通过重启主服务器并杀死备份来引发恢复。注意是客户端投票触发故障转移和恢复。

二进制星由有限状态机驱动。事件是对等状态，所以"对等活跃"意味着另一个服务器告诉我们它是活跃的。"客户端请求"意味着我们收到了客户端请求。"客户端投票"意味着我们收到了客户端请求且我们的对等服务器在两个心跳内不活跃。

注意服务器使用PUB-SUB套接字进行状态交换。没有其他套接字组合在这里工作。如果没有对等准备接收消息，PUSH和DEALER会阻塞。如果对等消失并回来，PAIR不会重新连接。ROUTER在向对等发送消息之前需要对等的地址。

### 二进制星反应器 {#Binary-Star-Reactor}

二进制星足够有用和通用，可以打包为可重用的反应器类。然后反应器运行并在有消息处理时调用我们的代码。这比将二进制星代码复制/粘贴到我们想要该功能的每个服务器中要好得多。

在C中，我们包装之前看到的CZMQ `zloop`类。`zloop`让你注册处理程序来对套接字和定时器事件做出反应。在二进制星反应器中，我们为投票者和状态变化（活跃到被动，反之亦然）提供处理程序。这是`bstar` API：

{{< fragment name="bstar" >}}
//  创建一个新的二进制星实例，使用本地（绑定）和
//  远程（连接）端点来设置服务器对等。
bstar_t *bstar_new (int primary, char *local, char *remote);

//  销毁二进制星实例
void bstar_destroy (bstar_t **self_p);

//  返回底层zloop反应器，用于定时器和读取器
//  注册和取消。
zloop_t *bstar_zloop (bstar_t *self);

//  注册投票读取器
int bstar_voter (bstar_t *self, char *endpoint, int type,
                 zloop_fn handler, void *arg);

//  注册主状态变化处理程序
void bstar_new_active (bstar_t *self, zloop_fn handler, void *arg);
void bstar_new_passive (bstar_t *self, zloop_fn handler, void *arg);

//  启动反应器，如果回调函数返回-1，或进程接收到SIGINT或SIGTERM，则结束。
int bstar_start (bstar_t *self);
{{< /fragment >}}

这是类实现：

{{< examples name="bstar" title="Binary Star core class" >}}

这给了我们以下服务器的简短主程序：

{{< examples name="bstarsrv2" title="Binary Star server, using core class" >}}

## 无代理可靠性（自由职业者模式） {#Brokerless-Reliability-Freelance-Pattern}

当我们经常将ZeroMQ解释为"无代理消息传递"时，如此专注于基于代理的可靠性可能看起来具有讽刺意味。然而，在消息传递中，如同在现实生活中，中间人既是负担也是益处。在实践中，大多数消息传递架构受益于分布式和代理消息传递的混合。当你可以自由决定你想要做什么权衡时，你得到最好的结果。这就是为什么我可以开车二十分钟去批发商为派对买五箱酒，但我也可以走十分钟去街角商店为晚餐买一瓶。我们对时间、精力和成本的高度上下文敏感的相对评估对现实世界经济至关重要。它们对最优的基于消息的架构也很重要。

这就是为什么ZeroMQ不*强加*以代理为中心的架构，尽管它确实给你构建代理（又名*代理*）的工具，我们到目前为止已经构建了十几个不同的代理，只是为了练习。

所以我们将通过解构到目前为止构建的基于代理的可靠性来结束这一章，并将其转回分布式点对点架构，我称之为自由职业者模式。我们的用例将是名称解析服务。这是ZeroMQ架构的一个常见问题：我们如何知道要连接的端点？在代码中硬编码TCP/IP地址是疯狂的脆弱。使用配置文件创建管理噩梦。想象一下，如果你必须在你使用的每台PC或手机上手动配置你的网络浏览器，以实现"google.com"是"74.125.230.82"。

ZeroMQ名称服务（我们将制作一个简单的实现）必须做以下事情：

* 将逻辑名称解析为至少一个绑定端点和一个连接端点。现实的名称服务将提供多个绑定端点，可能还有多个连接端点。

* 允许我们管理多个并行环境，例如"测试"与"生产"，而不修改代码。

* 是可靠的，因为如果它不可用，应用程序将无法连接到网络。

将名称服务放在面向服务的大管家代理后面从某些角度来看是聪明的。然而，将名称服务作为客户端可以直接连接的服务器暴露更简单且不那么令人惊讶。如果我们做得对，名称服务成为我们需要在代码或配置文件中硬编码的*唯一*全局网络端点。

我们旨在处理的故障类型是服务器崩溃和重启、服务器忙循环、服务器过载和网络问题。为了获得可靠性，我们将创建一个名称服务器池，如果一个崩溃或消失，客户端可以连接到另一个，等等。在实践中，两个就足够了。但对于这个例子，我们假设池可以是任何大小。

在这种架构中，大量客户端直接连接到少量服务器。服务器绑定到它们各自的地址。这与基于代理的方法（如大管家）根本不同，在那里工作者连接到代理。客户端有几个选择：

* 使用REQ套接字和懒惰海盗模式。简单，但需要一些额外的智能，这样客户端就不会愚蠢地一遍又一遍地尝试重新连接到死服务器。

* 使用DEALER套接字并爆发请求（将负载平衡到所有连接的服务器）直到它们得到回复。有效，但不优雅。

* 使用ROUTER套接字，这样客户端可以地址特定的服务器。但客户端如何知道服务器套接字的身份？要么服务器必须首先ping客户端（复杂），要么服务器必须使用客户端已知的硬编码、固定身份（讨厌）。

我们将在以下小节中开发这些中的每一个。

### 模型一：简单重试和故障转移 {#Model-One-Simple-Retry-and-Failover}

所以我们的菜单似乎提供：简单、残酷、复杂或讨厌。让我们从简单开始，然后解决问题。我们采用懒惰海盗并重写它以使用多个服务器端点。

首先启动一个或几个服务器，指定绑定端点作为参数：

{{< examples name="flserver1" title="Freelance server, Model One" >}}

然后启动客户端，指定一个或多个连接端点作为参数：

{{< examples name="flclient1" title="Freelance client, Model One" >}}

一个示例运行是：

```
flserver1 tcp://*:5555 &
flserver1 tcp://*:5556 &
flclient1 tcp://localhost:5555 tcp://localhost:5556
```

尽管基本方法是懒惰海盗，客户端旨在只获得一个成功的回复。它有两种技术，取决于你是运行单个服务器还是多个服务器：

* 对于单个服务器，客户端将重试几次，完全如懒惰海盗。
* 对于多个服务器，客户端将最多尝试每个服务器一次，直到它收到回复或已尝试所有服务器。

这解决了懒惰海盗的主要弱点，即它无法故障转移到备份或替代服务器。

然而，这种设计在真实应用程序中不会工作得很好。如果我们连接许多套接字而我们的主名称服务器下线，我们将每次都经历这种痛苦的超时。

### 模型二：残酷的霰弹枪大屠杀 {#Model-Two-Brutal-Shotgun-Massacre}

让我们将客户端切换到使用DEALER套接字。我们这里的目标是确保我们在尽可能短的时间内得到回复，无论特定服务器是否启动或关闭。我们的客户端采用这种方法：

* 我们设置事情，连接到所有服务器。
* 当我们有请求时，我们爆发它与我们有服务器一样多次。
* 我们等待第一个回复，并采用那个。
* 我们忽略任何其他回复。

在实践中将发生的是，当所有服务器运行时，ZeroMQ将分发请求，使每个服务器获得一个请求并发送一个回复。当任何服务器离线和断开连接时，ZeroMQ将分发请求到剩余的服务器。所以在某些情况下，服务器可能多次获得相同的请求。

对客户端更恼人的是，我们将得到多个回复，但不能保证我们将得到精确数量的回复。请求和回复可能丢失（例如，如果服务器在处理请求时崩溃）。

所以我们必须给请求编号并忽略任何与请求编号不匹配的回复。我们的模型一服务器将工作，因为它是一个echo服务器，但巧合不是理解的好基础。所以我们将制作一个模型二服务器，它咀嚼消息并返回内容为"OK"的正确编号回复。我们将使用由两部分组成的消息：序号和正文。

启动一个或多个服务器，每次指定一个绑定端点：

{{< examples name="flserver2" title="Freelance server, Model Two" >}}

然后启动客户端，指定连接端点作为参数：

{{< examples name="flclient2" title="Freelance client, Model Two" >}}

一个示例运行是：

```
flserver2 tcp://*:5555 &
flserver2 tcp://*:5556 &
flclient2 tcp://localhost:5555 tcp://localhost:5556
```

关于客户端实现的一些注意事项：

* 客户端结构为一个好的小基于类的API，隐藏创建ZeroMQ上下文和套接字以及与服务器交谈的肮脏工作。也就是说，如果对腹部的霰弹枪爆炸可以称为"交谈"。

* 如果客户端在几秒钟内找不到*任何*响应的服务器，它将放弃追逐。

* 客户端必须创建有效的REP信封，即在消息前面添加空消息帧。

客户端执行10,000次名称解析请求（假的，因为我们的服务器基本上什么都不做）并测量平均成本。在我的测试机上，与一个服务器交谈，这需要大约60微秒。与三个服务器交谈，大约需要80微秒。

我们霰弹枪方法的优缺点是：

* 优点：它简单，易于制作和理解。
* 优点：它完成故障转移的工作，快速工作，只要至少有一个服务器运行。
* 缺点：它创建冗余网络流量。
* 缺点：我们不能优先排序我们的服务器，即主要的，然后次要的。
* 缺点：服务器一次最多可以做一个请求，句号。

### 模型三：复杂和讨厌 {#Model-Three-Complex-and-Nasty}

霰弹枪方法似乎好得令人难以置信。让我们科学地工作通过所有替代方案。我们将探索复杂/讨厌的选择，即使只是最终意识到我们更喜欢残酷。啊，我生活的故事。

我们可以通过切换到ROUTER套接字来解决客户端的主要问题。这让我们向特定服务器发送请求，避免我们知道死了的服务器，一般来说，我们想要多聪明就多聪明。我们也可以通过切换到ROUTER套接字来解决服务器的主要问题（单线程性）。

但在两个匿名套接字（未设置身份）之间做ROUTER到ROUTER是不可能的。只有当它们接收到第一条消息时，双方才生成身份（对于另一个对等），因此直到它首先接收到消息，任何一方都不能与另一方交谈。摆脱这种困境的唯一方法是作弊，并在一个方向使用硬编码身份。在客户端/服务器情况下，正确的作弊方法是让客户端"知道"服务器的身份。反过来做将是疯狂的，在复杂和讨厌之上，因为任何数量的客户端应该能够独立产生。疯狂、复杂和讨厌对于种族灭绝独裁者来说是伟大的属性，但对于软件来说是可怕的。

而不是发明另一个要管理的概念，我们将使用连接端点作为身份。这是双方可以同意的唯一字符串，没有比霰弹枪模型已经拥有的更多先验知识。这是连接两个ROUTER套接字的狡猾和有效方法。

记住ZeroMQ身份如何工作。服务器ROUTER套接字在绑定其套接字之前设置身份。当客户端连接时，它们做一个小握手来交换身份，在任何一方发送真实消息之前。客户端ROUTER套接字，没有设置身份，向服务器发送空身份。服务器生成随机UUID来为其自己的使用指定客户端。服务器将其身份（我们已经同意将是端点字符串）发送给客户端。

这意味着我们的客户端可以一旦连接建立就将消息路由到服务器（即在其ROUTER套接字上发送，指定服务器端点作为身份）。这不是在做`zmq_connect()`之后*立即*，而是在那之后的某个随机时间。这里有一个问题：我们不知道服务器何时实际可用并完成其连接握手。如果服务器在线，可能在几毫秒后。如果服务器关闭且系统管理员外出吃午饭，可能从现在开始一小时。

这里有一个小悖论。我们需要知道服务器何时连接并可用于工作。在自由职业者模式中，与我们在本章早期看到的基于代理的模式不同，服务器在被说话之前是沉默的。因此，我们不能与服务器交谈，直到它告诉我们它在线，它不能这样做，直到我们问它。

我的解决方案是混合一点来自模型2的霰弹枪方法，意味着我们将向我们能做的任何事情开火（无害的）射击，如果有任何移动，我们知道它是活的。我们不打算开火真实请求，而是一种ping-pong心跳。

这使我们再次进入协议领域，所以这里是[定义自由职业者客户端和服务器如何交换ping-pong命令和请求-回复命令的简短规范](http://rfc.zeromq.org/spec:10)。

作为服务器实现它简短而甜美。这是我们的echo服务器，模型三，现在说FLP：

{{< examples name="flserver3" title="Freelance server, Model Three" >}}

然而，自由职业者客户端变得很大。为了清晰，它分为一个示例应用程序和一个做艰苦工作的类。这是顶级应用程序：

{{< examples name="flclient3" title="Freelance client, Model Three" >}}

这里，几乎和大管家代理一样复杂和大，是客户端API类：

{{< examples name="flcliapi" title="Freelance client API" >}}

这个API实现相当复杂，使用了我们以前没有见过的几种技术。

* **多线程API**：客户端API由两部分组成，在应用程序线程中运行的同步`flcliapi`类，和作为后台线程运行的异步*代理*类。记住ZeroMQ如何使创建多线程应用程序变得容易。flcliapi和代理类通过`inproc`套接字上的消息相互交谈。所有ZeroMQ方面（如创建和销毁上下文）都隐藏在API中。代理实际上像迷你代理一样行动，在后台与服务器交谈，这样当我们提出请求时，它可以尽最大努力到达它认为可用的服务器。

* **无滴答轮询定时器**：在以前的轮询循环中，我们总是使用固定滴答间隔，例如1秒，这足够简单但在功耗敏感的客户端（如笔记本或手机）上不是优秀的，唤醒CPU消耗电力。为了乐趣，并帮助拯救地球，代理使用*无滴答定时器*，它基于我们期望的下一个超时计算轮询延迟。正确的实现将保持超时的有序列表。我们只是检查所有超时并计算到下一个的轮询延迟。

## 结论 {#Conclusion}

在本章中，我们看到了各种可靠的请求-回复机制，每种都有一定的成本和好处。示例代码在很大程度上准备好用于真实使用，尽管它没有优化。在所有不同的模式中，用于生产使用的两个突出的是大管家模式，用于基于代理的可靠性，和自由职业者模式，用于无代理可靠性。
