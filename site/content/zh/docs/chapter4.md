---
weight: 4
title: '4. 可靠的请求-回复模式'
---

# 第4章 - 可靠的请求-回复模式 {#reliable-request-reply}

第3章涵盖了 ZeroMQ 请求-回复模式的高级用法，并提供了工作示例。本章着眼于可靠性的一般问题，并在 ZeroMQ 的核心请求-回复模式之上构建一套可靠的消息模式。

在本章中，我们重点关注用户空间请求-回复*模式*，可重用的模型，帮助你设计自己的 ZeroMQ 架构：

* *懒惰海盗*模式：从客户端进行可靠的请求-回复
* *简单海盗*模式：使用负载均衡的可靠请求-回复
* *偏执海盗*模式：带心跳的可靠请求-回复
* *大管家*模式：面向服务的可靠队列
* *泰坦尼克*模式：基于磁盘/断开连接的可靠队列
* *双星*模式：主备服务器故障转移
* *自由职业者*模式：无代理的可靠请求-回复

## 什么是"可靠性"？ {#What-is-Reliability}

大多数谈论"可靠性"的人实际上并不真正知道他们的意思。我们只能用故障来定义可靠性。也就是说，如果我们能够处理某一组明确定义和理解的故障，那么我们对于这些故障是可靠的。不多也不少。所以让我们看看分布式 ZeroMQ 应用程序中可能的故障原因，大致按照概率递减的顺序：

* 应用程序代码是最严重的问题。它可能崩溃和退出，冻结并停止响应输入，运行得太慢而无法处理输入，耗尽所有内存等等。

* 系统代码——比如我们使用 ZeroMQ 编写的代理——可能因为与应用程序代码相同的原因而死亡。系统代码*应该*比应用程序代码更可靠，但它仍然可能崩溃和烧毁，特别是如果它试图为慢客户端排队消息时可能耗尽内存。

* 消息队列可能溢出，通常发生在已经学会粗暴处理慢客户端的系统代码中。当队列溢出时，它开始丢弃消息。所以我们得到"丢失"的消息。

* 网络可能失败（例如，WiFi 被关闭或超出范围）。在这种情况下，ZeroMQ 会自动重新连接，但与此同时，消息可能会丢失。

* 硬件可能失败，并带走在该机器上运行的所有进程。

* 网络可能以奇特的方式失败，例如，交换机上的某些端口可能死亡，网络的那些部分变得无法访问。

* 整个数据中心可能被闪电、地震、火灾或更平凡的电力或冷却故障击中。

使软件系统对所有这些可能的故障完全可靠是一项极其困难和昂贵的工作，超出了本书的范围。

由于上述列表中的前五种情况涵盖了大公司之外99.9%的现实世界需求（根据我刚刚进行的高度科学研究，该研究还告诉我78%的统计数据是现场编造的，而且永远不要相信我们自己没有伪造的统计数据），这就是我们要检查的内容。如果你是一家有钱花在最后两种情况上的大公司，请立即联系我的公司！我海滩别墅后面有一个大洞等待改造成高管游泳池。

## 设计可靠性 {#Designing-Reliability}

所以为了让事情变得极其简单，可靠性就是"当代码冻结或崩溃时保持事情正常工作"，我们将这种情况简称为"死亡"。然而，我们想要保持正常工作的东西比仅仅消息更复杂。我们需要采用每个核心 ZeroMQ 消息模式，看看如何使其工作（如果我们可以的话），即使代码死亡。

让我们一一看看：

* 请求-回复：如果服务器死亡（在处理请求时），客户端可以发现这一点，因为它不会得到答案。然后它可以愤怒地放弃，等待并稍后重试，找到另一个服务器等等。至于客户端死亡，我们现在可以将其视为"别人的问题"。

* 发布-订阅：如果客户端死亡（在获得一些数据后），服务器不知道。发布-订阅不从客户端向服务器发送任何信息。但客户端可以带外联系服务器，例如，通过请求-回复，并问"请重新发送我错过的一切"。至于服务器死亡，这超出了这里的范围。订阅者也可以自我验证他们没有运行得太慢，如果他们是的话就采取行动（例如，警告操作员并死亡）。

* 管道：如果工作者死亡（在工作时），通风器不知道。管道，就像时间的研磨齿轮，只在一个方向工作。但下游收集器可以检测到一个任务没有完成，并向通风器发送消息说"嘿，重新发送任务324！"如果通风器或收集器死亡，最初发送工作批次的任何上游客户端可能厌倦等待并重新发送整批。这不优雅，但系统代码真的不应该经常死亡到足以重要。

在本章中，我们将只专注于请求-回复，这是可靠消息传递的低垂果实。

基本的请求-回复模式（REQ 客户端套接字对 REP 服务器套接字进行阻塞发送/接收）在处理最常见类型的故障方面得分很低。如果服务器在处理请求时崩溃，客户端只是永远挂起。如果网络丢失请求或回复，客户端永远挂起。

由于 ZeroMQ 能够静默重新连接对等体、负载均衡消息等，请求-回复仍然比 TCP 好得多。但对于真正的工作来说，它仍然不够好。你真正可以信任基本请求-回复模式的唯一情况是在同一进程中的两个线程之间，那里没有网络或单独的服务器进程会死亡。

然而，通过一点额外的工作，这个谦逊的模式成为跨分布式网络进行真正工作的良好基础，我们得到一套可靠的请求-回复（RRR）模式，我喜欢称之为*海盗*模式（我希望你最终会理解这个笑话）。

根据我的经验，大致有三种方式将客户端连接到服务器。每种都需要特定的可靠性方法：

* 多个客户端直接与单个服务器通话。用例：客户端需要与之通话的单个众所周知的服务器。我们旨在处理的故障类型：服务器崩溃和重启，以及网络断开。

* 多个客户端与将工作分发给多个工作者的代理代理通话。用例：面向服务的事务处理。我们旨在处理的故障类型：工作者崩溃和重启、工作者忙循环、工作者过载、队列崩溃和重启，以及网络断开。

* 多个客户端与多个服务器通话，没有中介代理。用例：分布式服务，如名称解析。我们旨在处理的故障类型：服务崩溃和重启、服务忙循环、服务过载，以及网络断开。

这些方法中的每一种都有其权衡，通常你会混合使用它们。我们将详细查看所有三种。

## 客户端可靠性（懒惰海盗模式） {#Client-Side-Reliability-Lazy-Pirate-Pattern}

我们可以通过对客户端的一些更改获得非常简单的可靠请求-回复。我们称之为懒惰海盗模式。我们不是进行阻塞接收，而是：

* 轮询 REQ 套接字，只有当确定回复已到达时才从中接收。
* 如果在超时期间内没有回复到达，则重新发送请求。
* 如果经过几个请求后仍然没有回复，则放弃事务。

如果你试图以严格的发送/接收方式之外的任何方式使用 REQ 套接字，你会得到错误（技术上，REQ 套接字实现一个小的有限状态机来强制发送/接收乒乓，因此错误代码被称为"EFSM"）。当我们想要在海盗模式中使用 REQ 时，这有点令人讨厌，因为我们可能在得到回复之前发送几个请求。

相当好的蛮力解决方案是在错误后关闭并重新打开 REQ 套接字：

```c
//  懒惰海盗客户端
//  使用 zmq_poll 来在超时时进行安全的请求-回复
//  运行此客户端与 hwserver 一起测试

#include "czmq.h"

#define REQUEST_TIMEOUT     2500    //  毫秒，（> 1000！）
#define REQUEST_RETRIES     3       //  放弃之前

static char *
s_recv_from_server (void *client)
{
    char *reply = NULL;
    
    //  轮询套接字进行回复，有超时
    zmq_pollitem_t items [] = { { client, 0, ZMQ_POLLIN, 0 } };
    int rc = zmq_poll (items, 1, REQUEST_TIMEOUT * ZMQ_POLL_MSEC);
    if (rc == -1)
        return NULL;    //  中断

    //  如果我们得到了回复，处理它
    if (items [0].revents & ZMQ_POLLIN)
        reply = zstr_recv (client);

    return reply;
}

int
main (void)
{
    zctx_t *ctx = zctx_new ();
    printf ("I: 连接到 hello world 服务器...\n");
    void *client = zsocket_new (ctx, ZMQ_REQ);
    assert (client);
    zsocket_connect (client, "tcp://localhost:5555");

    int sequence = 0;
    int retries_left = REQUEST_RETRIES;
    while (retries_left && !zctx_interrupted) {
        //  我们发送一个请求，然后我们得到一个回复
        char request [10];
        sprintf (request, "%d", ++sequence);
        zstr_send (client, request);

        char *reply = s_recv_from_server (client);
        if (reply) {
            printf ("I: 服务器回复了 (%s)\n", reply);
            free (reply);
            retries_left = REQUEST_RETRIES;
        }
        else {
            if (--retries_left == 0) {
                printf ("E: 服务器似乎离线了，正在放弃\n");
                break;
            }
            else {
                printf ("W: 服务器没有回复，正在重试...\n");
                //  旧套接字处于混乱状态，关闭它并打开新的
                zsocket_destroy (ctx, client);
                printf ("I: 重新连接到服务器...\n");
                client = zsocket_new (ctx, ZMQ_REQ);
                zsocket_connect (client, "tcp://localhost:5555");
            }
        }
    }
    zctx_destroy (&ctx);
    return 0;
}
```

与匹配的服务器一起运行：

```c
//  懒惰海盗服务器
//  绑定 REQ 套接字到 tcp://*:5555
//  就像 hwserver 一样，除了：
//   - 在某些周期后回显请求号
//   - 随机减慢回复或在某些周期后退出

#include "czmq.h"

int main (void)
{
    srandom ((unsigned) time (NULL));

    zctx_t *ctx = zctx_new ();
    void *server = zsocket_new (ctx, ZMQ_REP);
    zsocket_bind (server, "tcp://*:5555");

    int cycles = 0;
    while (1) {
        char *request = zstr_recv (server);
        cycles++;

        //  在某些周期后模拟各种问题
        if (cycles > 3 && randof (3) == 0) {
            printf ("I: 模拟崩溃\n");
            break;
        }
        else
        if (cycles > 3 && randof (3) == 0) {
            printf ("I: 模拟 CPU 过载\n");
            sleep (2);
        }
        printf ("I: 正常请求 (%s)\n", request);
        sleep (1);                  //  做一些工作
        zstr_send (server, request);
        free (request);
    }
    zctx_destroy (&ctx);
    return 0;
}
```

```
#-----------#   #-----------#   #-----------#
|  Client   |   |  Client   |   |  Client   |
+-----------+   +-----------+   +-----------+
|   Retry   |   |   Retry   |   |   Retry   |
+-----------+   +-----------+   +-----------+
|    REQ    |   |    REQ    |   |    REQ    |
'-----------'   '-----------'   '-----------'
      ^               ^               ^
      |               |               |
      '---------------+---------------'
                      |
                      v
                .----------.
                |   REP    |
                +----------+
                |  Server  |
                #----------#
```

要运行此测试用例，在两个控制台窗口中启动客户端和服务器。服务器将在几条消息后随机行为不当。你可以检查客户端的响应。这是来自服务器的典型输出：

```
I: normal request (1)
I: normal request (2)
I: normal request (3)
I: simulating CPU overload
I: normal request (4)
I: simulating a crash
```

这是客户端的响应：

```
I: connecting to server...
I: server replied OK (1)
I: server replied OK (2)
I: server replied OK (3)
W: no response from server, retrying...
I: connecting to server...
W: no response from server, retrying...
I: connecting to server...
E: server seems to be offline, abandoning
```

客户端为每条消息排序，并检查回复是否完全按顺序返回：没有请求或回复丢失，没有回复返回超过一次，或无序。运行测试几次，直到你确信这种机制实际有效。在生产应用程序中你不需要序列号；它们只是帮助我们信任我们的设计。

客户端使用 REQ 套接字，并进行蛮力关闭/重新打开，因为 REQ 套接字强加了严格的发送/接收周期。你可能会尝试使用 DEALER 代替，但这不是一个好决定。首先，这意味着模拟 REQ 对信封所做的秘密调味料（如果你忘记了那是什么，这是一个好兆头，表明你不想必须这样做）。其次，这意味着可能得到你不期望的回复。

仅在客户端处理故障在我们有一组客户端与单个服务器通话时有效。它可以处理服务器崩溃，但只有在恢复意味着重启同一服务器时。如果有永久错误，例如服务器硬件上的电源故障，这种方法不会工作。因为服务器中的应用程序代码通常是任何架构中最大的故障源，依赖单个服务器不是一个好主意。

所以，优缺点：

* 优点：易于理解和实现。
* 优点：与现有客户端和服务器应用程序代码配合良好。
* 优点：ZeroMQ 自动重试实际重新连接直到它工作。
* 缺点：不故障转移到备份或备用服务器。

## 基本可靠队列（简单海盗模式） {#Basic-Reliable-Queuing-Simple-Pirate-Pattern}

我们的第二种方法扩展了懒惰海盗模式，使用队列代理让我们透明地与多个服务器通话，我们可以更准确地称之为"工作者"。我们将分阶段开发这个，从最小工作模型开始，简单海盗模式。

在所有这些海盗模式中，工作者都是无状态的。如果应用程序需要一些共享状态，比如共享数据库，在我们设计消息传递框架时我们不知道它。有队列代理意味着工作者可以来去而客户端不知道任何关于它的事情。如果一个工作者死亡，另一个接管。这是一个很好的简单拓扑，只有一个真正的弱点，即中央队列本身，这可能成为管理问题和单点故障。

```
#-----------#   #-----------#   #-----------#
|  Client   |   |  Client   |   |  Client   |
+-----------+   +-----------+   +-----------+
|   Retry   |   |   Retry   |   |   Retry   |
+-----------+   +-----------+   +-----------+
|    REQ    |   |    REQ    |   |    REQ    |
'-----+-----'   '-----+-----'   '-----+-----'
      |               |               |
      '---------------+---------------'
                      |
                      v
                .-----------.
                |  ROUTER   |
                +-----------+
                |   Load    |
                |  balancer |
                +-----------+
                |  ROUTER   |
                '-----------'
                      ^
                      |
      .---------------+---------------.
      |               |               |
.-----+-----.   .-----+-----.   .-----+-----.
|    REQ    |   |    REQ    |   |    REQ    |
+-----------+   +-----------+   +-----------+
|   Worker  |   |   Worker  |   |   Worker  |
#-----------#   #-----------#   #-----------#
```

队列代理的基础是第3章中的负载均衡代理。我们需要做的绝对*最少*是什么来处理死亡或阻塞的工作者？结果，这出人意料地少。我们在客户端中已经有重试机制。所以使用负载均衡模式将工作得相当好。这符合 ZeroMQ 的哲学，即我们可以通过在中间插入朴素代理来扩展请求-回复等对等模式。

我们不需要特殊客户端；我们仍然使用懒惰海盗客户端。这是队列，它与负载均衡代理的主要任务相同：

```c
//  简单海盗队列
//  这与我们的负载均衡代理相同

#include "czmq.h"

int main (void)
{
    zctx_t *ctx = zctx_new ();

    //  前端套接字与客户端通话，后端与工作者通话
    void *frontend = zsocket_new (ctx, ZMQ_ROUTER);
    void *backend  = zsocket_new (ctx, ZMQ_ROUTER);
    zsocket_bind (frontend, "tcp://*:5555");    //  为客户端
    zsocket_bind (backend,  "tcp://*:5556");    //  为工作者

    //  工作者队列，每个条目是一个工作者身份
    zlist_t *workers = zlist_new ();

    while (true) {
        zmq_pollitem_t items [] = {
            { backend,  0, ZMQ_POLLIN, 0 },
            { frontend, 0, ZMQ_POLLIN, 0 }
        };
        //  只有工作者可用时才轮询前端
        int rc = zmq_poll (items, zlist_size (workers)? 2: 1, -1);
        if (rc == -1)
            break;              //  中断

        //  处理来自后端的工作者活动
        if (items [0].revents & ZMQ_POLLIN) {
            //  使用工作者身份进行负载均衡
            zmsg_t *msg = zmsg_recv (backend);
            if (!msg)
                break;          //  中断
            zframe_t *identity = zmsg_unwrap (msg);
            zlist_append (workers, identity);

            //  转发消息给客户端（如果需要）
            if (zmsg_size (msg) == 0)
                zmsg_destroy (&msg);
            else
                zmsg_send (&msg, frontend);
        }
        if (items [1].revents & ZMQ_POLLIN) {
            //  获取客户端请求，路由到第一个可用工作者
            zmsg_t *msg = zmsg_recv (frontend);
            if (msg) {
                zmsg_wrap (msg, (zframe_t *) zlist_pop (workers));
                zmsg_send (&msg, backend);
            }
        }
    }
    //  当我们完成时，清理正确
    while (zlist_size (workers)) {
        zframe_t *frame = (zframe_t *) zlist_pop (workers);
        zframe_destroy (&frame);
    }
    zlist_destroy (&workers);
    zctx_destroy (&ctx);
    return 0;
}
```

这是工作者，它采用懒惰海盗服务器并将其适配为负载均衡模式（使用 REQ "ready" 信号）：

```c
//  简单海盗工作者
//  连接 REQ 套接字到 tcp://localhost:5556
//  实现工作者部分的负载均衡

#include "czmq.h"

int main (void)
{
    zctx_t *ctx = zctx_new ();
    void *worker = zsocket_new (ctx, ZMQ_REQ);

    //  设置随机身份使跟踪更容易
    char identity [10];
    sprintf (identity, "%04X-%04X", randof (0x10000), randof (0x10000));
    zmq_setsockopt (worker, ZMQ_IDENTITY, identity, strlen (identity));
    zsocket_connect (worker, "tcp://localhost:5556");

    //  告诉代理我们准备好工作
    printf ("I: (%s) 工作者就绪\n", identity);
    zframe_t *frame = zframe_new (ZFRAME_MORE, 0);
    zframe_send (&frame, worker, 0);

    int cycles = 0;
    while (true) {
        zmsg_t *msg = zmsg_recv (worker);
        if (!msg)
            break;              //  中断

        //  在某些周期后模拟各种问题
        cycles++;
        if (cycles > 3 && randof (5) == 0) {
            printf ("I: (%s) 模拟崩溃\n", identity);
            zmsg_destroy (&msg);
            break;
        }
        else
        if (cycles > 3 && randof (5) == 0) {
            printf ("I: (%s) 模拟 CPU 过载\n", identity);
            sleep (3);
        }
        printf ("I: (%s) 正常回复\n", identity);
        sleep (1);              //  做一些工作
        zmsg_send (&msg, worker);
    }
    zctx_destroy (&ctx);
    return 0;
}
```

为了测试这个，启动一些工作者、一个懒惰海盗客户端和队列，以任何顺序。你会看到工作者最终都崩溃和烧毁，客户端重试然后放弃。队列从不停止，你可以无休止地重启工作者和客户端。这个模型与任意数量的客户端和工作者一起工作。

## 稳健可靠队列（偏执海盗模式） {#Robust-Reliable-Queuing-Paranoid-Pirate-Pattern}

```
#-----------#   #-----------#   #-----------#
|  Client   |   |  Client   |   |  Client   |
+-----------+   +-----------+   +-----------+
|   Retry   |   |   Retry   |   |   Retry   |
+-----------+   +-----------+   +-----------+
|    REQ    |   |    REQ    |   |    REQ    |
'-----+-----'   '-----+-----'   '-----+-----'
      |               |               |
      '---------------+---------------'
                      |
                      v
                .-----------.
                |  ROUTER   |
                +-----------+
                |   Queue   |
                +-----------+
                | Heartbeat |
                +-----------+
                |  ROUTER   |
                '-----------'
                      ^
                      |
      .---------------+---------------.
      |               |               |
.-----+-----.   .-----+-----.   .-----+-----.
|  DEALER   |   |  DEALER   |   |  DEALER   |
+-----------+   +-----------+   +-----------+
| Heartbeat |   | Heartbeat |   | Heartbeat |
+-----------+   +-----------+   +-----------+
|   Worker  |   |   Worker  |   |   Worker  |
#-----------#   #-----------#   #-----------#
```

简单海盗队列模式工作得相当好，特别是因为它只是两个现有模式的组合。不过，它确实有一些弱点：

* 面对队列崩溃和重启时它不稳健。客户端会恢复，但工作者不会。虽然 ZeroMQ 会自动重新连接工作者的套接字，但对于新启动的队列来说，工作者没有发出准备就绪信号，所以不存在。为了解决这个问题，我们必须从队列到工作者进行心跳，以便工作者可以检测到队列何时消失。

* 队列不检测工作者故障，所以如果工作者在空闲时死亡，队列无法将其从工作者队列中移除，直到队列向其发送请求。客户端等待并白白重试。这不是一个关键问题，但这不好。为了使这正常工作，我们从工作者到队列进行心跳，以便队列可以在任何阶段检测到丢失的工作者。

我们将在适当迂腐的偏执海盗模式中修复这些问题。

我们之前为工作者使用了 REQ 套接字。对于偏执海盗工作者，我们将切换到 DEALER 套接字。这有让我们随时发送和接收消息的优势，而不是 REQ 强加的锁步发送/接收。DEALER 的缺点是我们必须做自己的信封管理（重新阅读第3章以了解这个概念的背景）。

我们仍然使用懒惰海盗客户端。这是偏执海盗队列代理。由于代码相当长，我将展示核心概念：

偏执海盗队列扩展了带有工作者心跳的负载均衡模式。心跳是那些"简单"的事情之一，很难做对。当我写偏执海盗示例时，花了大约五个小时才让心跳正常工作。请求-回复链的其余部分可能花了十分钟。特别容易创建"虚假故障"，即当对等体因为心跳没有正确发送而决定它们断开连接时。

## 心跳 {#Heartbeating}

心跳解决了知道对等体是活着还是死亡的问题。这不是 ZeroMQ 特有的问题。TCP 有很长的超时（30分钟左右），这意味着可能无法知道对等体是否已经死亡、断开连接，或者带着一箱伏特加、一个红发女郎和一个大的费用账户去布拉格度周末。

正确进行心跳并不容易。在编写偏执海盗示例时，花了大约五个小时才让心跳正常工作。请求-回复链的其余部分可能花了十分钟。特别容易创建"虚假故障"，即当对等体因为心跳没有正确发送而决定它们断开连接时。

我们将看看人们在 ZeroMQ 心跳中使用的三个主要答案。

### 耸耸肩 {#Shrugging-It-Off}

最常见的方法是根本不进行心跳，并希望最好的结果。许多（如果不是大多数）ZeroMQ 应用程序都这样做。ZeroMQ 通过在许多情况下隐藏对等体来鼓励这一点。这种方法会导致什么问题？

* 当我们在跟踪对等体的应用程序中使用 ROUTER 套接字时，随着对等体断开连接和重新连接，应用程序将泄漏内存（应用程序为每个对等体持有的资源）并变得越来越慢。

* 当我们使用基于 SUB 或 DEALER 的数据接收者时，我们无法区分好的沉默（没有数据）和坏的沉默（另一端死亡）。当接收者知道另一边死亡时，它可以例如切换到备份路由。

* 如果我们使用长时间保持沉默的 TCP 连接，在某些网络中，它会死亡。发送某些东西（技术上，"保活"更多于心跳），将保持网络活跃。

### 单向心跳 {#One-Way-Heartbeats}

第二个选择是每秒左右从每个节点向其对等体发送心跳消息。当一个节点在某个超时内（通常几秒钟）没有听到另一个节点的消息时，它将把该对等体视为死亡。听起来不错，对吧？不幸的是，没有。这在某些情况下有效，但在其他情况下有令人讨厌的边缘情况。

对于发布-订阅，这确实有效，这是你可以使用的唯一模型。SUB 套接字不能与 PUB 套接字对话，但 PUB 套接字可以愉快地向其订阅者发送"我还活着"消息。

作为优化，你可以仅在没有真实数据要发送时才发送心跳。此外，如果网络活动是一个问题（例如，在移动网络上，活动会耗尽电池），你可以越来越慢地发送心跳。只要接收者可以检测到故障（活动的急剧停止），那就没问题。

这种设计的典型问题是：

* 当我们发送大量数据时，它可能不准确，因为心跳会在该数据后面延迟。如果心跳延迟，由于网络拥塞，你可能得到虚假超时和断开连接。因此，始终将*任何*传入数据视为心跳，无论发送者是否优化掉心跳。

* 虽然发布-订阅模式会为消失的接收者丢弃消息，但 PUSH 和 DEALER 套接字会排队它们。所以如果你向死亡的对等体发送心跳，当它回来时，它会得到你发送的所有心跳，这可能是数千个。哇哦！

* 这种设计假设心跳超时在整个网络中是相同的。但这不会准确。一些对等体会想要非常激进的心跳以便快速检测故障。一些会想要非常宽松的心跳，以便让睡眠网络安静并节省电力。

### 乒乓心跳 {#Ping-Pong-Heartbeats}

第三个选择是使用乒乓对话。一个对等体向另一个发送 ping 命令，后者用 pong 命令回复。两个命令都没有任何有效载荷。Ping 和 pong 没有关联。因为"客户端"和"服务器"的角色在某些网络中是任意的，我们通常指定任一对等体实际上都可以发送 ping 并期望响应中的 pong。然而，因为超时取决于动态客户端最了解的网络拓扑，通常是客户端 ping 服务器。

这适用于所有基于 ROUTER 的代理。我们在第二个模型中使用的相同优化使这工作得更好：将任何传入数据视为 pong，并且仅在不发送数据时才发送 ping。

### 偏执海盗的心跳 {#Heartbeating-for-Paranoid-Pirate}

对于偏执海盗，我们选择了第二种方法。这可能不是最简单的选择：如果今天设计这个，我可能会尝试乒乓方法。然而原则是相似的。心跳消息在两个方向异步流动，任一对等体都可以决定另一个"死亡"并停止与其交谈。

在工作者中，这是我们如何处理来自队列的心跳：

* 我们计算一个*活跃度*，这是在决定队列死亡之前我们仍然可以错过多少次心跳。它从三开始，每次我们错过心跳时递减。
* 我们在 `zmq_poll` 循环中等待，每次一秒，这是我们的心跳间隔。
* 如果在那段时间内来自队列有任何消息，我们将活跃度重置为三。
* 如果在那段时间内没有消息，我们倒计时我们的活跃度。
* 如果活跃度达到零，我们认为队列死亡。
* 如果队列死亡，我们销毁套接字，创建新的，并重新连接。
* 为了避免打开和关闭太多套接字，我们在重新连接之前等待一定间隔，每次将间隔加倍直到达到32秒。

这是我们如何处理*到*队列的心跳：

* 我们计算何时发送下一个心跳；这是单个变量，因为我们正在与一个对等体通话，队列。
* 在 `zmq_poll` 循环中，每当我们通过这个时间时，我们向队列发送心跳。

这是工作者的基本心跳代码：

```c
#define HEARTBEAT_LIVENESS  3       //  3-5 是合理的
#define HEARTBEAT_INTERVAL  1000    //  毫秒
#define INTERVAL_INIT       1000    //  初始重连
#define INTERVAL_MAX       32000    //  指数退避后

//  如果活跃度达到零，队列被认为断开连接
size_t liveness = HEARTBEAT_LIVENESS;
size_t interval = INTERVAL_INIT;

//  定期发送心跳
uint64_t heartbeat_at = zclock_time () + HEARTBEAT_INTERVAL;

while (true) {
    zmq_pollitem_t items [] = { { worker,  0, ZMQ_POLLIN, 0 } };
    int rc = zmq_poll (items, 1, HEARTBEAT_INTERVAL * ZMQ_POLL_MSEC);

    if (items [0].revents & ZMQ_POLLIN) {
        //  从队列接收任何消息
        liveness = HEARTBEAT_LIVENESS;
        interval = INTERVAL_INIT;
    }
    else
    if (--liveness == 0) {
        zclock_sleep (interval);
        if (interval < INTERVAL_MAX)
            interval *= 2;
        zsocket_destroy (ctx, worker);
        //  ...
        liveness = HEARTBEAT_LIVENESS;
    }
    //  如果是时候向队列发送心跳
    if (zclock_time () > heartbeat_at) {
        heartbeat_at = zclock_time () + HEARTBEAT_INTERVAL;
        //  向队列发送心跳消息
    }
}
```

队列做同样的事情，但管理每个工作者的过期时间。

以下是你自己心跳实现的一些提示：

* 使用 `zmq_poll` 或反应器作为应用程序主要任务的核心。

* 首先在对等体之间构建心跳，通过模拟故障来测试它，*然后*构建消息流的其余部分。事后添加心跳要棘手得多。

* 使用简单跟踪，即打印到控制台，来使这工作。为了帮助你跟踪对等体之间的消息流，使用像 zmsg 提供的转储方法，并递增编号你的消息，这样你就可以看到是否有间隙。

* 在真实应用程序中，心跳必须是可配置的，通常与对等体协商。一些对等体会想要激进的心跳，低至10毫秒。其他对等体会很远，想要心跳高达30秒。

* 如果你对不同对等体有不同的心跳间隔，你的轮询超时应该是这些的最低（最短时间）。不要使用无限超时。

* 在用于消息的同一套接字上进行心跳，这样你的心跳也作为*保活*防止网络连接变陈旧（一些防火墙对静默连接可能不友好）。

## 合同和协议 {#Contracts-and-Protocols}

如果你注意的话，你会意识到由于心跳，偏执海盗与简单海盗不可互操作。但我们如何定义"可互操作"？为了保证互操作性，我们需要一种合同，一种让不同时间和地点的不同团队编写保证一起工作的代码的协议。我们称之为"协议"。

没有规范进行实验很有趣，但这不是真实应用程序的明智基础。如果我们想用另一种语言编写工作者会怎样？我们必须读代码来看事情如何工作吗？如果我们因为某种原因想改变协议会怎样？即使是简单的协议，如果它成功，也会演变并变得更复杂。

缺少合同是一次性应用程序的确定迹象。所以让我们为这个协议写一个合同。我们怎么做？

在 [http://rfc.zeromq.org rfc.zeromq.org] 有一个 wiki，我们特别制作作为公共 ZeroMQ 合同的家。要创建新规范，如果需要在 wiki 上注册，并按照说明操作。这相当直接，尽管写技术文本不是每个人都喜欢的。

我花了大约十五分钟起草新的[海盗模式协议](http://rfc.zeromq.org/spec:6)。这不是一个大规范，但它确实捕获了足够作为争论的基础（"你的队列不兼容 PPP；请修复它！"）。

将 PPP 转变为真正的协议需要更多工作：

* READY 命令中应该有协议版本号，以便可能区分不同版本的 PPP。

* 现在，READY 和 HEARTBEAT 与请求和回复并不完全不同。为了使它们不同，我们需要包含"消息类型"部分的消息结构。

## 面向服务的可靠队列（大管家模式） {#Service-Oriented-Reliable-Queuing-Majordomo-Pattern}

```
#-----------#   #-----------#   #-----------#
|  Client   |   |  Client   |   |  Client   |
'-----+-----'   '-----+-----'   '-----+-----'
      |               |               |
      '---------------+---------------'
"Give me coffee"      |         "Give me tea"
                      v
                .-----------.
                |  Broker   |
                '-----------'
                      ^
                      |
      .---------------+---------------.
      |               |               |
.-----+-----.   .-----+-----.   .-----+-----.
|  "Water"  |   |   "Tea"   |   | "Coffee"  |
+-----------+   +-----------+   +-----------+
|  Worker   |   |  Worker   |   |  Worker   |
#-----------#   #-----------#   #-----------#
```

进步的好处是当律师和委员会不参与时它发生得多快。[单页 MDP 规范](http://rfc.zeromq.org/spec:7)将 PPP 变成更坚固的东西。这是我们应该设计复杂架构的方式：首先写下合同，*然后*编写软件来实现它们。

大管家协议（MDP）以一种有趣的方式扩展和改进了 PPP：它向客户端发送的请求添加"服务名称"，并要求工作者注册特定服务。添加服务名称将我们的偏执海盗队列变成面向服务的代理。MDP 的好处是它来自工作代码、更简单的祖先协议（PPP）和一组精确的改进，每个都解决了明确的问题。这使得起草变得容易。

为了实现大管家，我们需要为客户端和工作者编写框架。要求每个应用程序开发人员阅读规范并使其工作，当他们可以使用为他们工作的更简单 API 时，这真的不明智。

所以虽然我们的第一个合同（MDP本身）定义了我们分布式架构的部分如何彼此交谈，我们的第二个合同定义了用户应用程序如何与我们要设计的技术框架交谈。

大管家有两半，客户端侧和工作者侧。因为我们将编写客户端和工作者应用程序，我们需要两个 API。这是客户端 API 的草图，使用简单的面向对象方法：

```c
mdcli_t *mdcli_new     (char *broker);
void     mdcli_destroy (mdcli_t **self_p);
zmsg_t  *mdcli_send    (mdcli_t *self, char *service, zmsg_t **request_p);
```

就是这样。我们打开到代理的会话，发送请求消息，得到回复消息，最终关闭连接。这是工作者 API 的草图：

```c
mdwrk_t *mdwrk_new     (char *broker,char *service);
void     mdwrk_destroy (mdwrk_t **self_p);
zmsg_t  *mdwrk_recv    (mdwrk_t *self, zmsg_t *reply);
```

它或多或少是对称的，但工作者对话有点不同。工作者第一次做 recv() 时，它传递空回复。此后，它传递当前回复，并得到新请求。

客户端和工作者 API 相当容易构建，因为它们很大程度上基于我们已经开发的偏执海盗代码。

让我们看看客户端 API 在行动中的样子，有一个做10万次请求-回复循环的示例测试程序：

```c
//  大管家协议客户端示例
//  使用 mdcli API 隐藏所有 MDP 协议复杂性

#include "mdcliapi.h"

int main (int argc, char *argv [])
{
    int verbose = (argc > 1 && streq (argv [1], "-v"));
    mdcli_t *session = mdcli_new ("tcp://localhost:5555", verbose);

    int count;
    for (count = 0; count < 100000; count++) {
        zmsg_t *request = zmsg_new ();
        zmsg_pushstr (request, "Hello world");
        zmsg_t *reply = mdcli_send (session, "echo", &request);
        if (reply)
            zmsg_destroy (&reply);
        else
            break;              //  由于中断，中断请求-回复
    }
    printf ("%d 请求/回复已处理\n", count);
    mdcli_destroy (&session);
    return 0;
}
```

让我们看看工作者 API 在行动中的样子，有一个实现回声服务的示例测试程序：

```c
//  大管家协议工作者示例
//  使用 mdwrk API 隐藏所有 MDP 协议复杂性

#include "mdwrkapi.h"

int main (int argc, char *argv [])
{
    int verbose = (argc > 1 && streq (argv [1], "-v"));
    mdwrk_t *session = mdwrk_new (
        "tcp://localhost:5555", "echo", verbose);

    zmsg_t *reply = NULL;
    while (true) {
        zmsg_t *request = mdwrk_recv (session, &reply);
        if (request == NULL)
            break;              //  由于中断，工作者被杀死
        reply = request;        //  回声很简单
    }
    mdwrk_destroy (&session);
    return 0;
}
```

关于工作者 API 代码需要注意的一些事情：

* API 是单线程的。例如，这意味着工作者不会在后台发送心跳。令人高兴的是，这正是我们想要的：如果工作者应用程序卡住，心跳将停止，代理将停止向工作者发送请求。

* 工作者 API 不做指数退避；额外的复杂性不值得。

* API 不做任何错误报告。如果某些东西不如期望，它们引发断言（或异常，取决于语言）。这对参考实现是理想的，所以任何协议错误立即显示。对于真实应用程序，API 应该对无效消息稳健。

你可能想知道为什么工作者 API 手动关闭其套接字并打开新的，当如果对等体消失并回来 ZeroMQ 会自动重新连接套接字。回顾简单海盗和偏执海盗工作者来理解。虽然如果代理死亡并回来 ZeroMQ 会自动重新连接工作者，这不足以用代理重新注册工作者。我知道至少两个解决方案。最简单的，我们在这里使用，是工作者使用心跳监控连接，如果它决定代理死亡，关闭其套接字并用新套接字重新开始。替代方案是代理在从工作者得到心跳时挑战未知工作者并要求它们重新注册。这需要协议支持。

现在让我们设计大管家代理。其核心结构是一组队列，每个服务一个。我们将在工作者出现时创建这些队列（我们可以在工作者消失时删除它们，但现在忘记这个，因为它变得复杂）。此外，我们为每个服务保持工作者队列。

这是代理的核心实现概念。这是迄今为止我们见过的最复杂的示例。完整的大管家代理实现接近500行代码。编写这个并使其有些稳健花了两天。然而，对于全面向服务代理来说，这仍然是一段简短的代码。
