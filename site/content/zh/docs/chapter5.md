---
weight: 5
title: '5. 高级发布-订阅模式'
---

# 第5章 - 高级发布-订阅模式 {#advanced-pub-sub}

在[第3章 - 高级请求-回复模式](chapter3#advanced-request-reply)和[第4章 - 可靠的请求-回复模式](chapter4#reliable-request-reply)中，我们研究了ZeroMQ请求-回复模式的高级用法。如果你成功消化了所有这些内容，恭喜你。在本章中，我们将专注于发布-订阅，并通过更高级的性能、可靠性、状态分发和监控模式来扩展ZeroMQ的核心发布-订阅模式。

我们将涵盖：

* 何时使用发布-订阅
* 如何处理太慢的订阅者（*自杀蜗牛*模式）
* 如何设计高速订阅者（*黑盒*模式）
* 如何监控发布-订阅网络（*浓缩咖啡*模式）
* 如何构建共享键值存储（*克隆*模式）
* 如何使用反应器简化复杂服务器
* 如何使用二进制星模式为服务器添加故障转移

## 发布-订阅的优缺点 {#Pros-and-Cons-of-Pub-Sub}

ZeroMQ的底层模式有其不同的特性。发布-订阅解决了一个古老的消息传递问题，即*多播*或*组消息传递*。它具有ZeroMQ特有的细致简单性和残酷冷漠的独特混合。值得理解发布-订阅所做的权衡，这些权衡如何使我们受益，以及如果需要的话我们如何解决它们。

首先，PUB将每条消息发送给"许多中的所有"，而PUSH和DEALER将消息轮换到"许多中的一个"。你不能简单地用PUB替换PUSH，反之亦然，并希望事情能正常工作。这值得重复，因为人们似乎经常建议这样做。

更深刻的是，发布-订阅旨在可扩展性。这意味着大量数据，快速发送给许多接收者。如果你需要每秒向数千个点发送数百万条消息，你会比每秒向少数接收者发送几条消息更欣赏发布-订阅。

为了获得可扩展性，发布-订阅使用与推拉相同的技巧，即摆脱反向聊天。这意味着接收者不会回复发送者。有一些例外，例如SUB套接字会向PUB套接字发送订阅，但它是匿名和不频繁的。

消除反向聊天对于真正的可扩展性至关重要。对于发布-订阅，这就是模式如何干净地映射到PGM多播协议，该协议由网络交换机处理。换句话说，订阅者根本不连接到发布者，它们连接到交换机上的多播*组*，发布者向其发送消息。

当我们移除反向聊天时，我们的整体消息流变得*更加*简单，这让我们能够制作更简单的API、更简单的协议，并且通常接触到更多的人。但我们也移除了协调发送者和接收者的任何可能性。这意味着：

* 发布者无法知道订阅者何时成功连接，无论是在初始连接时，还是在网络故障后的重连时。

* 订阅者无法告诉发布者任何允许发布者控制它们发送消息速率的信息。发布者只有一个设置，即*全速*，订阅者必须要么跟上，要么丢失消息。

* 发布者无法知道订阅者何时因进程崩溃、网络中断等而消失。

缺点是，如果我们想要进行可靠的多播，我们实际上需要所有这些。当订阅者连接时、网络故障发生时，或者如果订阅者或网络无法跟上发布者时，ZeroMQ发布-订阅模式将任意丢失消息。

优点是有许多用例，*几乎*可靠的多播就很好。当我们需要这种反向聊天时，我们可以切换到使用ROUTER-DEALER（我倾向于在大多数正常音量情况下这样做），或者我们可以添加一个单独的同步通道（我们将在本章后面看到一个例子）。

发布-订阅就像广播电台；你错过了加入之前的一切，然后你获得多少信息取决于你的接收质量。令人惊讶的是，这个模型是有用和广泛的，因为它完美地映射到现实世界的信息分发。想想Facebook和Twitter、BBC世界服务和体育结果。

正如我们对请求-回复所做的那样，让我们根据可能出错的情况来定义*可靠性*。以下是发布-订阅的经典故障情况：

* 订阅者加入太晚，所以他们错过了服务器已经发送的消息。
* 订阅者获取消息太慢，所以队列建立然后溢出。
* 订阅者可能掉线并在离开时丢失消息。
* 订阅者可能崩溃并重启，并丢失它们已经接收的任何数据。
* 网络可能变得过载并丢弃数据（特别是对于PGM）。
* 网络可能变得太慢，所以发布者端队列溢出，发布者崩溃。

可能出错的更多，但这些是我们在现实系统中看到的典型故障。自v3.x以来，ZeroMQ对其内部缓冲区强制执行默认限制（所谓的高水位标记或HWM），所以除非你故意将HWM设置为无限，否则发布者崩溃更少见。

所有这些故障情况都有答案，尽管不总是简单的。可靠性需要我们大多数人大部分时间不需要的复杂性，这就是为什么ZeroMQ不尝试开箱即用地提供它（即使有一个全局的可靠性设计，也没有）。

## 发布-订阅跟踪（浓缩咖啡模式） {#Pub-Sub-Tracing-Espresso-Pattern}

让我们通过查看跟踪发布-订阅网络的方法来开始本章。在[第2章 - 套接字和模式](chapter2#sockets-and-patterns)中，我们看到了一个使用这些来进行传输桥接的简单代理。`zmq_proxy()`方法有三个参数：它桥接在一起的*前端*和*后端*套接字，以及一个它将发送所有消息的*捕获*套接字。

代码看起来简单得令人怀疑：

{{< examples name="espresso" title="Espresso Pattern" >}}

浓缩咖啡通过创建一个监听器线程来工作，该线程读取PAIR套接字并打印它获得的任何内容。那个PAIR套接字是管道的一端；另一端（另一个PAIR）是我们传递给`zmq_proxy()`的套接字。在实践中，你会过滤有趣的消息以获得你想要跟踪的本质（因此模式的名称）。

订阅者线程订阅"A"和"B"，接收五条消息，然后销毁其套接字。当你运行示例时，监听器打印两条订阅消息、五条数据消息、两条取消订阅消息，然后静默：

```
[002] 0141
[002] 0142
[007] B-91164
[007] B-12979
[007] A-52599
[007] A-06417
[007] A-45770
[002] 0041
[002] 0042
```

这很好地显示了当没有订阅者时，发布者套接字如何停止发送数据。发布者线程仍在发送消息。套接字只是静默地丢弃它们。

## 最后值缓存 {#Last-Value-Caching}

如果你使用过商业发布-订阅系统，你可能习惯了快速和愉快的ZeroMQ发布-订阅模型中缺少的一些功能。其中之一是*最后值缓存*（LVC）。这解决了新订阅者加入网络时如何追赶的问题。理论是当新订阅者加入并订阅某些特定主题时，发布者会得到通知。然后发布者可以重新广播这些主题的最后消息。

我已经解释了为什么发布者在有新订阅者时不会得到通知，因为在大型发布-订阅系统中，数据量使其几乎不可能。要制作真正大规模的发布-订阅网络，你需要像PGM这样的协议，它利用大规模以太网交换机向数千个订阅者多播数据的能力。尝试从发布者到数千个订阅者中的每一个进行TCP单播根本无法扩展。你会得到奇怪的尖峰、不公平的分发（一些订阅者比其他订阅者更早得到消息）、网络拥塞和普遍的不快。

PGM是单向协议：发布者向交换机的多播地址发送消息，然后重新广播给所有感兴趣的订阅者。发布者从不看到订阅者何时加入或离开：这一切都发生在交换机中，我们真的不想开始重新编程。

然而，在具有几十个订阅者和有限数量主题的低音量网络中，我们可以使用TCP，然后XSUB和XPUB套接字*确实*相互交谈，正如我们刚才在浓缩咖啡模式中看到的。

我们能用ZeroMQ制作LVC吗？答案是肯定的，如果我们制作一个位于发布者和订阅者之间的代理；PGM交换机的类比，但我们可以自己编程。

我将首先制作一个发布者和订阅者，突出最坏的情况。这个发布者是病态的。它首先立即向一千个主题中的每一个发送消息，然后每秒向随机主题发送一次更新。订阅者连接并订阅一个主题。没有LVC，订阅者必须平均等待500秒才能获得任何数据。为了增加一些戏剧性，让我们假装有一个名叫格雷戈尔的逃犯威胁要撕掉玩具兔子罗杰的头，如果我们不能修复那8.3分钟的延迟。

这是发布者代码。注意它有命令行选项连接到某个地址，但否则绑定到端点。我们稍后将使用它连接到我们的最后值缓存：

{{< examples name="pathopub" title="Pathologic Publisher" >}}

这是订阅者：

{{< examples name="pathosub" title="Pathologic Subscriber" >}}

尝试构建和运行这些：首先是订阅者，然后是发布者。你会看到订阅者报告得到"Save Roger"，如你所期望的：

```
./pathosub &
./pathopub
```

当你运行第二个订阅者时，你就理解了罗杰的困境。你必须等很长时间才能报告得到任何数据。所以，这是我们的最后值缓存。正如我承诺的，它是一个绑定到两个套接字然后处理两者上的消息的代理：

{{< examples name="lvcache" title="Last Value Caching Proxy" >}}

现在，运行代理，然后是发布者：

```
./lvcache &
./pathopub tcp://localhost:5557
```

现在运行任意数量的订阅者实例，每次连接到端口5558上的代理：

```
./pathosub tcp://localhost:5558
```

每个订阅者都愉快地报告"Save Roger"，格雷戈尔逃犯溜回他的座位吃晚饭和一杯热牛奶，这正是他首先真正想要的。

一个注意事项：默认情况下，XPUB套接字不报告重复订阅，这是当你天真地将XPUB连接到XSUB时你想要的。我们的示例巧妙地通过使用随机主题来解决这个问题，所以它不工作的机会是百万分之一。在真实的LVC代理中，你会想要使用我们在[第6章 - ZeroMQ社区](chapter6#the-community)中作为练习实现的`ZMQ_XPUB_VERBOSE`选项。

## 慢订阅者检测（自杀蜗牛模式） {#Slow-Subscriber-Detection-Suicidal-Snail-Pattern}

在现实生活中使用发布-订阅模式时，你会遇到的一个常见问题是慢订阅者。在理想世界中，我们以全速从发布者向订阅者流传输数据。在现实中，订阅者应用程序通常用解释语言编写，或者只是做很多工作，或者只是写得很糟糕，以至于它们无法跟上发布者。

我们如何处理慢订阅者？理想的修复是使订阅者更快，但这可能需要工作和时间。处理慢订阅者的一些经典策略是：

* **在发布者上排队消息**。这是当我几个小时不读邮件时Gmail所做的。但在高音量消息传递中，向上游推送队列有令人兴奋但无利可图的结果，即使发布者内存不足并崩溃——特别是如果有很多订阅者并且出于性能原因无法刷新到磁盘。

* **在订阅者上排队消息**。这要好得多，如果网络能跟上事情，这是ZeroMQ默认做的。如果有人将内存不足并崩溃，那将是订阅者而不是发布者，这是公平的。这对于订阅者在一段时间内无法跟上但在流减慢时可以追赶的"尖峰"流来说是完美的。然而，这对于通常太慢的订阅者来说不是答案。

* **一段时间后停止排队新消息**。这是当我的邮箱溢出其珍贵的千兆字节空间时Gmail所做的。新消息只是被拒绝或丢弃。从发布者的角度来看，这是一个很好的策略，当发布者设置HWM时，这是ZeroMQ所做的。然而，它仍然不能帮助我们修复慢订阅者。现在我们只是在消息流中得到间隙。

* **用断连惩罚慢订阅者**。这是当我两周没有登录时Hotmail（还记得吗？）所做的，这就是为什么当我意识到也许有更好的方法时，我在我的第十五个Hotmail账户上。这是一个很好的残酷策略，迫使订阅者坐起来注意，将是理想的，但ZeroMQ不这样做，没有办法在上面分层，因为订阅者对发布者应用程序是不可见的。

这些经典策略都不合适，所以我们需要发挥创造力。而不是断开发布者，让我们说服订阅者杀死自己。这是自杀蜗牛模式。当订阅者检测到它运行太慢时（其中"太慢"大概是一个配置选项，真正意味着"如此慢以至于如果你到这里，大声喊叫，因为我需要知道，所以我可以修复这个！"），它呱呱叫并死亡。

订阅者如何检测到这个？一种方法是对消息进行排序（按顺序编号）并在发布者使用HWM。现在，如果订阅者检测到间隙（即编号不连续），它知道出了问题。然后我们将HWM调整到"如果你达到这个就呱呱叫并死亡"级别。

这个解决方案有两个问题。一，如果我们有许多发布者，我们如何对消息进行排序？解决方案是给每个发布者一个唯一的ID并将其添加到排序中。二，如果订阅者使用`ZMQ_SUBSCRIBE`过滤器，它们根据定义会得到间隙。我们珍贵的排序将毫无用处。

一些用例不会使用过滤器，排序对它们有效。但更一般的解决方案是发布者给每条消息加时间戳。当订阅者得到消息时，它检查时间，如果差异超过，比如说，一秒，它做"呱呱叫并死亡"的事情，可能首先向某个操作员控制台发出尖叫。

自杀蜗牛模式特别适用于订阅者有自己的客户端和服务级别协议并需要保证某些最大延迟的情况。中止订阅者可能看起来不像保证最大延迟的建设性方法，但这是断言模型。今天中止，问题将被修复。允许延迟数据向下游流动，问题可能造成更广泛的损害并需要更长时间才能出现在雷达上。

这是自杀蜗牛的最小示例：

{{< examples name="suisnail" title="Suicidal Snail" >}}

关于自杀蜗牛示例需要注意的一些事情：

* 这里的消息简单地由当前系统时钟作为毫秒数组成。在现实应用程序中，你至少会有带时间戳的消息头和带数据的消息体。

* 示例在单个进程中将订阅者和发布者作为两个线程。在现实中，它们将是单独的进程。使用线程只是为了演示方便。

## 高速订阅者（黑盒模式） {#High-Speed-Subscribers-Black-Box-Pattern}

现在让我们看看使我们的订阅者更快的一种方法。发布-订阅的一个常见用例是分发大数据流，如来自证券交易所的市场数据。典型的设置将有一个连接到证券交易所的发布者，获取价格报价，并将它们发送给多个订阅者。如果有少数订阅者，我们可以使用TCP。如果我们有更多订阅者，我们可能会使用可靠的多播，即PGM。

让我们想象我们的feed平均每秒有100,000条100字节的消息。这是一个典型的速率，在过滤掉我们不需要发送给订阅者的市场数据之后。现在我们决定记录一天的数据（也许8小时内250 GB），然后将其重放到模拟网络，即一小组订阅者。虽然每秒100K消息对ZeroMQ应用程序来说很容易，但我们想要*更快*地重放它。

所以我们设置我们的架构，有一堆盒子——一个给发布者，一个给每个订阅者。这些是规格良好的盒子——八个内核，十二个给发布者。

当我们向订阅者泵入数据时，我们注意到两件事：

1. 当我们对消息做哪怕最轻微的工作时，它会减慢我们的订阅者，以至于它无法再次追赶发布者。

1. 我们在发布者和订阅者都达到了天花板，大约每秒6M消息，即使在仔细优化和TCP调优之后。

我们必须做的第一件事是将我们的订阅者分解为多线程设计，这样我们可以在一组线程中处理消息，同时在另一组线程中读取消息。通常，我们不想以相同方式处理每条消息。相反，订阅者将过滤一些消息，也许通过前缀键。当消息匹配某些标准时，订阅者将调用工作者来处理它。在ZeroMQ术语中，这意味着将消息发送给工作者线程。

所以订阅者看起来像队列设备。我们可以使用各种套接字连接订阅者和工作者。如果我们假设单向流量和所有相同的工作者，我们可以使用PUSH和PULL并将所有路由工作委托给ZeroMQ。这是最简单和最快的方法。

订阅者通过TCP或PGM与发布者交谈。订阅者与其工作者交谈，它们都在同一个进程中，通过`inproc:`。

现在打破那个天花板。订阅者线程达到100% CPU，因为它是一个线程，它不能使用超过一个内核。单个线程总是会达到天花板，无论是在2M、6M还是更多每秒消息。我们想要跨多个可以并行运行的线程分割工作。

许多高性能产品使用的方法，在这里有效，是*分片*。使用分片，我们将工作分割为并行和独立的流，例如一半的主题键在一个流中，一半在另一个流中。我们可以使用许多流，但除非我们有空闲内核，否则性能不会扩展。所以让我们看看如何分片为两个流。

有两个流，以全速工作，我们将配置ZeroMQ如下：

* 两个I/O线程，而不是一个。
* 两个网络接口（NIC），每个订阅者一个。
* 每个I/O线程绑定到特定NIC。
* 两个订阅者线程，绑定到特定内核。
* 两个SUB套接字，每个订阅者线程一个。
* 其余内核分配给工作者线程。
* 工作者线程连接到两个订阅者PUSH套接字。

理想情况下，我们想要匹配我们架构中完全加载线程的数量与内核的数量。当线程开始为内核和CPU周期而战时，添加更多线程的成本超过了好处。例如，创建更多I/O线程没有好处。

## 可靠的发布-订阅（克隆模式） {#Reliable-Pub-Sub-Clone-Pattern}

作为一个更大的工作示例，我们将解决制作可靠发布-订阅架构的问题。我们将分阶段开发这个。目标是允许一组应用程序共享一些公共状态。这是我们的技术挑战：

* 我们有大量客户端应用程序，比如数千或数万。
* 它们将任意加入和离开网络。
* 这些应用程序必须共享单一的最终一致*状态*。
* 任何应用程序都可以在任何时间点更新状态。

假设更新是相当低音量的。我们没有实时目标。整个状态可以放入内存。一些合理的用例是：

* 一组云服务器共享的配置。
* 一组玩家共享的一些游戏状态。
* 实时更新并对应用程序可用的汇率数据。

### 集中式与分散式 {#Centralized-Versus-Decentralized}

我们必须做的第一个决定是我们是否使用中央服务器。这在结果设计中产生很大差异。权衡是这些：

* 概念上，中央服务器更容易理解，因为网络不是自然对称的。有了中央服务器，我们避免了所有发现、绑定与连接等问题。

* 通常，完全分布式架构在技术上更具挑战性，但最终有更简单的协议。也就是说，每个节点必须以正确的方式充当服务器和客户端，这很微妙。做得对，结果比使用中央服务器更简单。我们在[第4章 - 可靠的请求-回复模式](chapter4#reliable-request-reply)的自由职业者模式中看到了这一点。

* 在高音量用例中，中央服务器将成为瓶颈。如果需要处理每秒数百万消息的规模，我们应该立即瞄准去中心化。

* 讽刺的是，集中式架构将比分散式架构更容易扩展到更多节点。也就是说，将10,000个节点连接到一个服务器比相互连接更容易。

所以，对于克隆模式，我们将使用发布状态更新的*服务器*和代表应用程序的一组*客户端*。

### 将状态表示为键值对 {#Representing-State-as-Key-Value-Pairs}

我们将分阶段开发克隆，一次解决一个问题。首先，让我们看看如何在一组客户端之间更新共享状态。我们需要决定如何表示我们的状态以及更新。最简单的合理格式是键值存储，其中一个键值对表示共享状态中的原子更改单位。

我们在[第1章 - 基础](chapter1#basics)中有一个简单的发布-订阅示例，天气服务器和客户端。让我们改变服务器发送键值对，客户端将这些存储在哈希表中。这让我们使用经典的发布-订阅模型从一个服务器向一组客户端发送更新。

更新要么是新的键值对，要么是现有键的修改值，要么是删除的键。我们现在可以假设整个存储适合内存，应用程序通过键访问它，例如使用哈希表或字典。对于更大的存储和某种持久性，我们可能会将状态存储在数据库中，但这在这里不相关。

这是服务器：

{{< examples name="clonesrv1" title="Clone server, Model One" >}}

这是客户端：

{{< examples name="clonecli1" title="Clone client, Model One" >}}

关于这第一个模型需要注意的一些事情：

* 所有艰苦的工作都在`kvmsg`类中完成。这个类处理键值消息对象，它们是结构为三帧的多部分ZeroMQ消息：键（ZeroMQ字符串）、序列号（64位值，网络字节顺序）和二进制正文（保存其他所有内容）。

* 服务器生成带有随机4位键的消息，这让我们模拟一个大但不巨大的哈希表（10K条目）。

* 我们在这个版本中不实现删除：所有消息都是插入或更新。

* 服务器在绑定其套接字后暂停200毫秒。这是为了防止*慢加入者综合症*，其中订阅者在连接到服务器套接字时丢失消息。我们将在克隆代码的后续版本中移除这个。

* 我们将在代码中使用术语*发布者*和*订阅者*来指代套接字。这将在后面当我们有多个套接字做不同事情时有所帮助。

这是最简单的`kvmsg`类，现在可以工作：

{{< examples name="kvsimple" title="Key-value message class" >}}

稍后，我们将制作一个更复杂的`kvmsg`类，它将在真实应用程序中工作。

服务器和客户端都维护哈希表，但这第一个模型只有在我们在服务器之前启动所有客户端且客户端从不崩溃时才能正常工作。这是非常人为的。

### 获取带外快照 {#Getting-an-Out-of-Band-Snapshot}

所以现在我们有了第二个问题：如何处理迟到的客户端或崩溃然后重启的客户端。

为了允许迟到（或恢复）的客户端追赶服务器，它必须获得服务器状态的快照。正如我们将"消息"简化为"有序键值对"，我们可以将"状态"简化为"哈希表"。要获得服务器状态，客户端打开DEALER套接字并明确请求它。

为了使这个工作，我们必须解决时间问题。获得状态快照需要一定时间，如果快照很大，可能相当长。我们需要正确地将更新应用到快照。但服务器不知道何时开始向我们发送更新。一种方式是开始订阅，获得第一次更新，然后要求"更新N的状态"。这将要求服务器为每次更新存储一个快照，这是不实用的。

所以我们将在客户端中进行同步，如下：

* 客户端首先订阅更新，然后发出状态请求。这保证状态将比它拥有的最旧更新更新。

* 客户端等待服务器用状态回复，同时排队所有更新。它简单地通过不读取它们来做到这一点：ZeroMQ将它们保持在套接字队列中排队。

* 当客户端接收到其状态更新时，它再次开始读取更新。但是，它丢弃任何比状态更新更旧的更新。所以如果状态更新包括到200的更新，客户端将丢弃到201的更新。

* 然后客户端将更新应用到其自己的状态快照。

这是一个利用ZeroMQ自己内部队列的简单模型。这是服务器：

{{< examples name="clonesrv2" title="Clone server, Model Two" >}}

这是客户端：

{{< examples name="clonecli2" title="Clone client, Model Two" >}}

关于这两个程序需要注意的一些事情：

* 服务器使用两个任务。一个线程（随机）产生更新并将这些发送到主PUB套接字，而另一个线程处理ROUTER套接字上的状态请求。两者通过PAIR套接字在`inproc:`连接上通信。

* 客户端真的很简单。在C中，它包含大约五十行代码。大量繁重的工作在`kvmsg`类中完成。即便如此，基本的克隆模式比最初看起来更容易实现。

* 我们不使用任何花哨的东西来序列化状态。哈希表保存一组`kvmsg`对象，服务器将这些作为消息批次发送给请求状态的客户端。如果多个客户端同时请求状态，每个都将获得不同的快照。

* 我们假设客户端有确切的一个服务器来交谈。服务器必须运行；我们不尝试解决如果服务器崩溃会发生什么的问题。

现在，这两个程序不做任何真实的事情，但它们正确地同步状态。这是一个如何混合不同模式的好例子：PAIR-PAIR、PUB-SUB和ROUTER-DEALER。

### 从客户端重新发布更新 {#Republishing-Updates-from-Clients}

在我们的第二个模型中，对键值存储的更改来自服务器本身。这是一个集中式模型，例如如果我们有一个想要分发的中央配置文件，在每个节点上有本地缓存，这是有用的。一个更有趣的模型从客户端而不是服务器获取更新。因此服务器变成无状态代理。这给我们一些好处：

* 我们不太担心服务器的可靠性。如果它崩溃，我们可以启动新实例并向其提供新值。

* 我们可以使用键值存储在活跃对等体之间分享知识。

要从客户端向服务器发送更新，我们可以使用各种套接字模式。最简单的合理解决方案是PUSH-PULL组合。

为什么我们不允许客户端直接相互发布更新？虽然这会减少延迟，但它会移除一致性保证。如果你允许更新的顺序根据谁接收它们而改变，你就无法获得一致的共享状态。假设我们有两个客户端，改变不同的键。这将工作得很好。但如果两个客户端尝试大致同时改变相同的键，它们最终会有不同的值概念。

当更改在多个地方同时发生时，有几种获得一致性的策略。我们将使用集中所有更改的方法。无论客户端进行更改的精确时间如何，它们都通过服务器推送，服务器根据它获得更新的顺序强制执行单一序列。

通过调解所有更改，服务器还可以为所有更新添加唯一序列号。有了唯一排序，客户端可以检测到更讨厌的故障，包括网络拥塞和队列溢出。如果客户端发现其传入消息流有漏洞，它可以采取行动。客户端联系服务器并要求缺失消息似乎是明智的，但在实践中这是无用的。如果有漏洞，它们是由网络压力引起的，向网络添加更多压力将使事情变得更糟。客户端所能做的就是警告其用户它"无法继续"，停止，并且在有人手动检查问题原因之前不重启。

我们现在将在客户端中生成状态更新。这是服务器：

{{< examples name="clonesrv3" title="Clone server, Model Three" >}}

这是客户端：

{{< examples name="clonecli3" title="Clone client, Model Three" >}}

关于这第三个设计需要注意的一些事情：

* 服务器已经折叠为单一任务。它管理用于传入更新的PULL套接字、用于状态请求的ROUTER套接字和用于传出更新的PUB套接字。

* 客户端使用简单的无滴答定时器每秒向服务器发送随机更新。在真实实现中，我们将从应用程序代码驱动更新。

### 使用子树 {#Working-with-Subtrees}

随着我们增加客户端数量，我们共享存储的大小也会增长。向每个客户端发送所有内容不再合理。这是发布-订阅的经典故事：当你有非常少的客户端时，你可以向所有客户端发送每条消息。随着你增长架构，这变得低效。客户端专门在不同区域。

所以即使在使用共享存储时，一些客户端将希望只使用该存储的一部分，我们称之为*子树*。客户端在发出状态请求时必须请求子树，并且在订阅更新时必须指定相同的子树。

树有几种常见语法。一种是*路径层次结构*，另一种是*主题树*。这些看起来像：

* 路径层次结构：`/some/list/of/paths`
* 主题树：`some.list.of.topics`

我们将使用路径层次结构，并扩展我们的客户端和服务器，使客户端可以使用单个子树。一旦你看到如何使用单个子树，你就能够自己扩展这个以处理多个子树，如果你的用例需要的话。

这是实现子树的服务器，模型三的小变化：

{{< examples name="clonesrv4" title="Clone server, Model Four" >}}

这是相应的客户端：

{{< examples name="clonecli4" title="Clone client, Model Four" >}}

### 临时值 {#Ephemeral-Values}

临时值是自动过期的值，除非定期刷新。如果你认为克隆被用于注册服务，那么临时值将让你做动态值。节点加入网络，发布其地址，并定期刷新这个。如果节点死亡，其地址最终被移除。

临时值的通常抽象是将它们附加到*会话*，并在会话结束时删除它们。在克隆中，会话将由客户端定义，如果客户端死亡将结束。一个更简单的替代方案是将*生存时间*（TTL）附加到临时值，服务器使用它来使未及时刷新的值过期。

我尽可能使用的一个好设计原则是*不发明不是绝对必要的概念*。如果我们有非常大量的临时值，会话将提供更好的性能。如果我们使用少量临时值，在每个上设置TTL是可以的。如果我们使用大量临时值，将它们附加到会话并批量使它们过期更有效。这不是我们在这个阶段面临的问题，可能永远不会面临，所以会话被抛弃。

现在我们将实现临时值。首先，我们需要一种在键值消息中编码TTL的方法。我们可以添加一个帧。使用ZeroMQ帧作为属性的问题是，每次我们想要添加新属性时，我们必须更改消息结构。它破坏兼容性。所以让我们向消息添加属性帧，并编写代码让我们获取和放置属性值。

接下来，我们需要一种说"删除这个值"的方法。到目前为止，服务器和客户端总是盲目地将新值插入或更新到它们的哈希表中。我们将说如果值为空，那意味着"删除这个键"。

这是`kvmsg`类的更完整版本，它实现属性帧（并添加UUID帧，我们稍后需要）。如果需要，它还通过从哈希中删除键来处理空值：

{{< examples name="kvmsg" title="Key-value message class: full" >}}

模型五客户端几乎与模型四相同。它现在使用完整的`kvmsg`类，并在每条消息上设置随机`ttl`属性（以秒为单位测量）：

{{< fragment name="kvsetttl" >}}
kvmsg_set_prop (kvmsg, "ttl", "%d", randof (30));
{{< /fragment >}}

### 使用反应器 {#Using-a-Reactor}

到目前为止，我们在服务器中使用了轮询循环。在服务器的下一个模型中，我们切换到使用反应器。在C中，我们使用CZMQ的`zloop`类。使用反应器使代码更冗长，但更容易理解和构建，因为服务器的每个部分都由单独的反应器处理程序处理。

我们使用单个线程并将服务器对象传递给反应器处理程序。我们可以将服务器组织为多个线程，每个处理一个套接字或定时器，但当线程不必共享数据时这工作得更好。在这种情况下，所有工作都围绕服务器的哈希映射，所以一个线程更简单。

有三个反应器处理程序：

* 一个处理来自ROUTER套接字的快照请求；
* 一个处理来自客户端的传入更新，来自PULL套接字；
* 一个使超过TTL的临时值过期。

{{< examples name="clonesrv5" title="Clone server, Model Five" >}}

### 为可靠性添加二进制星模式 {#Adding-the-Binary-Star-Pattern-for-Reliability}

到目前为止我们探索的克隆模型相对简单。现在我们要进入令人不快的复杂领域，这让我起来再喝一杯浓缩咖啡。你应该意识到制作"可靠"消息传递足够复杂，你总是需要问"我们真的需要这个吗？"在投入之前。如果你可以逃脱不可靠或"足够好"的可靠性，你可以在成本和复杂性方面获得巨大胜利。当然，你可能偶尔丢失一些数据。这通常是一个好的权衡。话虽如此，和...啜饮...因为浓缩咖啡真的很好，让我们跳进去。

当你玩最后一个模型时，你会停止并重启服务器。它可能看起来像恢复了，但当然它将更新应用到空状态而不是适当的当前状态。任何加入网络的新客户端只会获得最新更新而不是完整的历史记录。

我们想要的是服务器从被杀死或崩溃中恢复的方法。我们还需要提供备份，以防服务器离开服务任何时间长度。当有人要求"可靠性"时，要求他们列出他们想要处理的故障。在我们的情况下，这些是：

* 服务器进程崩溃并自动或手动重启。进程失去其状态并必须从某处取回。

* 服务器机器死亡并离线很长时间。客户端必须切换到某处的替代服务器。

* 服务器进程或机器与网络断开连接，例如交换机死亡或数据中心被敲出。它可能在某个时候回来，但与此同时客户端需要替代服务器。

我们的第一步是添加第二个服务器。我们可以使用[第4章 - 可靠的请求-回复模式](chapter4#reliable-request-reply)的二进制星模式将这些组织为主要和备份。二进制星是反应器，所以我们已经将最后一个服务器模型重构为反应器风格是有用的。

我们需要确保如果主服务器崩溃，更新不会丢失。最简单的技术是将它们发送到两个服务器。然后备份服务器可以充当客户端，并通过接收更新来保持其状态同步，就像所有客户端一样。它还将从客户端获得新更新。它还不能将这些存储在其哈希表中，但它可以保持一段时间。

所以，模型六在模型五基础上引入以下更改：

* 我们使用发布-订阅流而不是推拉流来发送客户端更新到服务器。这处理将更新扇出到两个服务器。否则我们必须使用两个DEALER套接字。

* 我们向服务器更新（到客户端）添加心跳，这样客户端可以检测主服务器何时死亡。然后它可以切换到备份服务器。

* 我们使用二进制星`bstar`反应器类连接两个服务器。二进制星依赖客户端通过向它们认为活跃的服务器发出明确请求来投票。我们将使用快照请求作为投票机制。

* 我们通过添加UUID字段使所有更新消息唯一可识别。客户端生成这个，服务器在重新发布的更新上传播它。

* 被动服务器保持它从客户端接收但尚未从活跃服务器接收的更新的"待处理列表"；或它从活跃服务器接收但尚未从客户端接收的更新。列表从最旧到最新排序，这样很容易从头部移除更新。

客户端逻辑设计为有限状态机是有用的。客户端循环三个状态：

* 客户端打开并连接其套接字，然后从第一个服务器请求快照。为了避免请求风暴，它只会询问任何给定服务器两次。一个请求可能丢失，这将是坏运气。两个丢失将是粗心。

* 客户端等待来自当前服务器的回复（快照数据），如果得到它，它存储它。如果在某个超时内没有回复，它故障转移到下一个服务器。

* 当客户端获得其快照时，它等待并处理更新。再次，如果它在某个超时内没有从服务器听到任何东西，它故障转移到下一个服务器。

客户端永远循环。在启动或故障转移期间，一些客户端可能试图与主服务器交谈，而其他客户端试图与备份服务器交谈，这是很可能的。二进制星状态机处理这个，希望准确。很难证明软件正确；相反我们敲打它直到我们不能证明它错误。

故障转移如下发生：

* 客户端检测到主服务器不再发送心跳，并得出结论它已经死亡。客户端连接到备份服务器并请求新的状态快照。

* 备份服务器开始从客户端接收快照请求，并检测到主服务器已经消失，所以它接管为主。

* 备份服务器将其待处理列表应用到其自己的哈希表，然后开始处理状态快照请求。

当主服务器重新上线时，它将：

* 作为被动服务器启动，并作为克隆客户端连接到备份服务器。

* 开始通过其SUB套接字从客户端接收更新。

我们做一些假设：

* 至少一个服务器将保持运行。如果两个服务器都崩溃，我们失去所有服务器状态，没有办法恢复它。

* 多个客户端不会同时更新相同的哈希键。客户端更新将以不同顺序到达两个服务器。因此，备份服务器可能以与主服务器不同的顺序应用其待处理列表的更新。来自一个客户端的更新将始终以相同顺序到达两个服务器，所以这是安全的。

因此，我们使用二进制星模式的高可用性服务器对的架构有两个服务器和一组与两个服务器交谈的客户端。

这是克隆服务器的第六个也是最后一个模型：

{{< examples name="clonesrv6" title="Clone server, Model Six" >}}

这个模型只有几百行代码，但花了相当长时间才让它工作。准确地说，构建模型六花了大约一整周的"甜美的上帝，这对于一个例子来说太复杂了"攻击。我们已经将几乎所有东西和厨房水槽组装到这个小应用程序中。我们有故障转移、临时值、子树等等。让我惊讶的是，前期设计相当准确。尽管如此，编写和调试如此多套接字流的细节相当具有挑战性。

基于反应器的设计从代码中移除了大量繁重工作，剩下的更简单、更容易理解。我们重用了[第4章 - 可靠的请求-回复模式](chapter4#reliable-request-reply)的bstar反应器。整个服务器作为一个线程运行，所以没有线程间的奇怪之处——只是一个结构指针（`self`）传递给所有处理程序，它们可以愉快地做它们的事情。使用反应器的一个好副作用是代码，不那么紧密集成到轮询循环中，更容易重用。模型六的大块取自模型五。

我一点一点地构建它，让每个部分*正确*工作，然后转到下一个。因为有四到五个主要套接字流，这意味着相当多的调试和测试。我只是通过向控制台转_dump_消息来调试。不要使用经典调试器来逐步通过ZeroMQ应用程序；你需要看到消息流来理解正在发生的事情。

对于测试，我总是尝试使用Valgrind，它捕获内存泄漏和无效内存访问。在C中，这是一个主要关注点，因为你不能委托给垃圾收集器。使用适当和一致的抽象，如kvmsg和CZMQ，有极大帮助。

### 集群哈希映射协议 {#The-Clustered-Hashmap-Protocol}

虽然服务器几乎是前一个模型加上二进制星模式的混搭，但客户端要复杂得多。但在我们进入那个之前，让我们看看最终协议。我已经将其作为ZeroMQ RFC网站上的规范写出，作为[集群哈希映射协议](http://rfc.zeromq.org/spec:12)。

粗略地说，有两种方式设计这样的复杂协议。一种方式是将每个流分离为其自己的套接字集。这是我们这里使用的方法。优点是每个流简单清洁。缺点是同时管理多个套接字流可能相当复杂。使用反应器使其更简单，但仍然，它产生许多必须正确组合的移动部件。

制作这样协议的第二种方式是为所有内容使用单个套接字对。在这种情况下，我会为服务器使用ROUTER，为客户端使用DEALER，然后通过该连接做所有事情。它产生更复杂的协议，但至少复杂性都在一个地方。在[第7章 - 使用ZeroMQ的高级架构](chapter7#advanced-architecture)中，我们将看一个通过ROUTER-DEALER组合完成的协议的例子。

让我们看看CHP规范。注意"SHOULD"、"MUST"和"MAY"是我们在协议规范中使用的关键词，以指示需求级别。

**目标**

CHP意在为通过ZeroMQ网络连接的客户端集群提供可靠发布-订阅的基础。它定义由键值对组成的"哈希映射"抽象。任何客户端都可以随时修改任何键值对，更改传播到所有客户端。客户端可以随时加入网络。

**架构**

CHP连接一组客户端应用程序和一组服务器。客户端连接到服务器。客户端互相看不到。客户端可以任意来去。

**端口和连接**

服务器必须如下打开三个端口：

* 端口号P的SNAPSHOT端口（ZeroMQ ROUTER套接字）。
* 端口号P + 1的PUBLISHER端口（ZeroMQ PUB套接字）。
* 端口号P + 2的COLLECTOR端口（ZeroMQ SUB套接字）。

客户端应该至少打开两个连接：

* 到端口号P的SNAPSHOT连接（ZeroMQ DEALER套接字）。
* 到端口号P + 1的SUBSCRIBER连接（ZeroMQ SUB套接字）。

如果客户端想要更新哈希映射，它可以打开第三个连接：

* 到端口号P + 2的PUBLISHER连接（ZeroMQ PUB套接字）。

这个额外帧在下面解释的命令中没有显示。

**状态同步**

客户端必须通过向其快照连接发送ICANHAZ命令来开始。这个命令由如下两帧组成：

```
ICANHAZ命令
-----------------------------------
帧0: "ICANHAZ?"
帧1: 子树规范
```

两帧都是ZeroMQ字符串。子树规范可能为空。如果不为空，它由斜杠后跟一个或多个路径段组成，以斜杠结尾。

服务器必须通过向其快照端口发送零个或多个KVSYNC命令，然后发送KTHXBAI命令来响应ICANHAZ命令。服务器必须为每个命令加上客户端的身份前缀，由ZeroMQ与ICANHAZ命令一起提供。KVSYNC命令指定单个键值对如下：

```
KVSYNC命令
-----------------------------------
帧0: 键，作为ZeroMQ字符串
帧1: 序列号，网络顺序8字节
帧2: <空>
帧3: <空>
帧4: 值，作为blob
```

序列号没有意义，可能为零。

KTHXBAI命令采用这种形式：

```
KTHXBAI命令
-----------------------------------
帧0: "KTHXBAI"
帧1: 序列号，网络顺序8字节
帧2: <空>
帧3: <空>
帧4: 子树规范
```

序列号必须是先前发送的KVSYNC命令的最高序列号。

当客户端接收到KTHXBAI命令时，它应该开始从其订阅者连接接收消息并应用它们。

**服务器到客户端更新**

当服务器有其哈希映射的更新时，它必须在其发布者套接字上将此作为KVPUB命令广播。KVPUB命令有这种形式：

```
KVPUB命令
-----------------------------------
帧0: 键，作为ZeroMQ字符串
帧1: 序列号，网络顺序8字节
帧2: UUID，16字节
帧3: 属性，作为ZeroMQ字符串
帧4: 值，作为blob
```

序列号必须严格递增。客户端必须丢弃序列号不严格大于接收到的最后KTHXBAI或KVPUB命令的任何KVPUB命令。

UUID是可选的，帧2可能为空（大小为零）。属性字段格式为零个或多个"name=value"实例，后跟换行符。如果键值对没有属性，属性字段为空。

如果值为空，客户端应该删除具有指定键的键值条目。

在没有其他更新的情况下，服务器应该定期发送HUGZ命令，例如每秒一次。HUGZ命令有这种格式：

```
HUGZ命令
-----------------------------------
帧0: "HUGZ"
帧1: 00000000
帧2: <空>
帧3: <空>
帧4: <空>
```

客户端可以将HUGZ的缺失视为服务器已崩溃的指示器（见下面的可靠性）。

**客户端到服务器更新**

当客户端有其哈希映射的更新时，它可以通过其发布者连接将此作为KVSET命令发送到服务器。KVSET命令有这种形式：

```
KVSET命令
-----------------------------------
帧0: 键，作为ZeroMQ字符串
帧1: 序列号，网络顺序8字节
帧2: UUID，16字节
帧3: 属性，作为ZeroMQ字符串
帧4: 值，作为blob
```

序列号没有意义，可能为零。如果使用可靠的服务器架构，UUID应该是通用唯一标识符。

如果值为空，服务器必须删除具有指定键的键值条目。

服务器应该接受以下属性：

* `ttl`：指定以秒为单位的生存时间。如果KVSET命令有`ttl`属性，服务器应该删除键值对并广播带有空值的KVPUB以在TTL过期时从所有客户端删除此项。

**可靠性**

CHP可以在双服务器配置中使用，其中如果主服务器失败，备份服务器接管。CHP不指定用于此故障转移的机制，但二进制星模式可能有帮助。

为了协助服务器可靠性，客户端可以：

* 在每个KVSET命令中设置UUID。
* 检测在一段时间内缺少HUGZ，并将此用作当前服务器已失败的指示器。
* 连接到备份服务器并重新请求状态同步。

**可扩展性和性能**

CHP设计为可扩展到大量（数千）客户端，仅受代理上系统资源限制。因为所有更新通过单个服务器，峰值时总吞吐量将限制在每秒数百万次更新，可能更少。

**安全性**

CHP不实现任何认证、访问控制或加密机制，不应在需要这些的任何部署中使用。

### 构建多线程堆栈和API {#Building-a-Multithreaded-Stack-and-API}

到目前为止我们使用的客户端堆栈不够智能，无法正确处理此协议。一旦我们开始做心跳，我们需要一个可以在后台线程中运行的客户端堆栈。在[第4章 - 可靠的请求-回复模式](chapter4#reliable-request-reply)结尾的自由职业者模式中，我们使用了多线程API但没有详细解释它。事实证明，当你开始制作像CHP这样更复杂的ZeroMQ协议时，多线程API非常有用。

如果你制作非平凡协议并期望应用程序正确实现它，大多数开发人员在大多数时候会搞错。你会留下很多不快乐的人抱怨你的协议太复杂、太脆弱、太难使用。而如果你给他们一个简单的API来调用，你有一些机会让他们买入。

我们的多线程API由前端对象和后台代理组成，由两个PAIR套接字连接。像这样连接两个PAIR套接字非常有用，你的高级绑定应该可能做CZMQ所做的，即打包"创建带有我可以用来向其发送消息的管道的新线程"方法。

我们在本书中看到的多线程API都采用相同形式：

* 对象的构造函数（`clone_new`）创建上下文并启动连接管道的后台线程。它持有管道的一端，这样它可以向后台线程发送命令。

* 后台线程启动*代理*，它本质上是从管道套接字和任何其他套接字（这里，DEALER和SUB套接字）读取的`zmq_poll`循环。

* 主应用程序线程和后台线程现在仅通过ZeroMQ消息通信。按惯例，前端发送字符串命令，这样类上的每个方法都变成发送到后端代理的消息，像这样：

{{< fragment name="connect-command" >}}
void
clone_connect (clone_t *self, char *address, char *service)
{
    assert (self);
    zmsg_t *msg = zmsg_new ();
    zmsg_addstr (msg, "CONNECT");
    zmsg_addstr (msg, address);
    zmsg_addstr (msg, service);
    zmsg_send (&msg, self->pipe);
}
{{< /fragment >}}

* 如果方法需要返回代码，它可以等待来自代理的回复消息。

* 如果代理需要向前端发送异步事件，我们向类添加`recv`方法，它等待前端管道上的消息。

* 我们可能想要暴露前端管道套接字句柄以允许类集成到进一步的轮询循环中。否则任何`recv`方法将阻塞应用程序。

克隆类有与[第4章 - 可靠的请求-回复模式](chapter4#reliable-request-reply)的`flcliapi`类相同的结构，并添加来自克隆客户端最后模型的逻辑。没有ZeroMQ，这种多线程API设计将是几周真正艰苦的工作。有了ZeroMQ，这是一两天的工作。

克隆类的实际API方法非常简单：

{{< fragment name="clone-methods" >}}
//  创建新的克隆类实例
clone_t *
    clone_new (void);

//  销毁克隆类实例
void
    clone_destroy (clone_t **self_p);

//  为此克隆类定义子树（如果有）
void
    clone_subtree (clone_t *self, char *subtree);

//  将克隆类连接到一个服务器
void
    clone_connect (clone_t *self, char *address, char *service);

//  在共享哈希映射中设置值
void
    clone_set (clone_t *self, char *key, char *value, int ttl);

//  从共享哈希映射获取值
char *
    clone_get (clone_t *self, char *key);
{{< /fragment >}}

所以这是克隆客户端的模型六，它现在已经变成只是使用克隆类的瘦壳：

{{< examples name="clonecli6" title="Clone client, Model Six" >}}

注意连接方法，它指定一个服务器端点。在幕后，我们实际上在与三个端口交谈。然而，正如CHP协议所说，三个端口在连续端口号上：

* 服务器状态路由器（ROUTER）在端口P。
* 服务器更新发布者（PUB）在端口P + 1。
* 服务器更新订阅者（SUB）在端口P + 2。

所以我们可以将三个连接折叠为一个逻辑操作（我们实现为三个单独的ZeroMQ连接调用）。

让我们以克隆堆栈的源代码结束。这是一段复杂的代码，但当你将其分解为前端对象类和后端代理时更容易理解。前端向代理发送字符串命令（"SUBTREE"、"CONNECT"、"SET"、"GET"），代理处理这些命令以及与服务器交谈。这是代理的逻辑：

1. 通过从第一个服务器获取快照开始
1. 当我们获得快照时切换到从订阅者套接字读取。
1. 如果我们没有获得快照，则故障转移到第二个服务器。
1. 在管道和订阅者套接字上轮询。
1. 如果我们在管道上有输入，处理来自前端对象的控制消息。
1. 如果我们在订阅者上有输入，存储或应用更新。
1. 如果我们在一定时间内没有从服务器获得任何东西，故障转移。
1. 重复直到进程被Ctrl-C中断。

这是实际的克隆类实现：

{{< examples name="clone" title="Clone class" >}}
